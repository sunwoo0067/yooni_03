name: Performance Monitoring

on:
  schedule:
    - cron: '0 */6 * * *'  # 6ì‹œê°„ë§ˆë‹¤ ì‹¤í–‰
  workflow_dispatch:
    inputs:
      test_type:
        description: 'Type of performance test'
        required: true
        default: 'load'
        type: choice
        options:
          - load
          - stress
          - spike
          - soak

env:
  TARGET_URL: ${{ secrets.STAGING_URL || 'http://localhost:8000' }}

jobs:
  # ë¶€í•˜ í…ŒìŠ¤íŠ¸
  load-test:
    runs-on: ubuntu-latest
    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Set up k6
      run: |
        sudo gpg --no-default-keyring --keyring /usr/share/keyrings/k6-archive-keyring.gpg --keyserver hkp://keyserver.ubuntu.com:80 --recv-keys C5AD17C747E3415A3642D57D77C6C491D6AC1D69
        echo "deb [signed-by=/usr/share/keyrings/k6-archive-keyring.gpg] https://dl.k6.io/deb stable main" | sudo tee /etc/apt/sources.list.d/k6.list
        sudo apt-get update
        sudo apt-get install k6

    - name: Prepare test scripts
      run: |
        mkdir -p scripts/performance
        cat > scripts/performance/load-test.js << 'EOF'
        import http from 'k6/http';
        import { check, sleep } from 'k6';
        import { Rate } from 'k6/metrics';

        const errorRate = new Rate('errors');

        export const options = {
          stages: [
            { duration: '5m', target: 100 },   // Ramp up
            { duration: '10m', target: 100 },  // Stay at 100 users
            { duration: '5m', target: 200 },   // Ramp up to 200
            { duration: '10m', target: 200 },  // Stay at 200 users
            { duration: '5m', target: 0 },     // Ramp down
          ],
          thresholds: {
            http_req_duration: ['p(95)<500', 'p(99)<1000'],
            http_req_failed: ['rate<0.1'],
            errors: ['rate<0.1'],
          },
        };

        export default function () {
          // ìƒí’ˆ ëª©ë¡ ì¡°íšŒ
          let res = http.get(`${__ENV.TARGET_URL}/api/v1/products`);
          check(res, {
            'status is 200': (r) => r.status === 200,
            'response time < 500ms': (r) => r.timings.duration < 500,
          }) || errorRate.add(1);
          
          sleep(1);
          
          // ìƒí’ˆ ê²€ìƒ‰
          res = http.get(`${__ENV.TARGET_URL}/api/v1/products/search?q=test`);
          check(res, {
            'search status is 200': (r) => r.status === 200,
          }) || errorRate.add(1);
          
          sleep(1);
          
          // ìƒí’ˆ ìƒì„¸ ì¡°íšŒ
          if (res.status === 200 && res.json().results && res.json().results.length > 0) {
            const productId = res.json().results[0].id;
            res = http.get(`${__ENV.TARGET_URL}/api/v1/products/${productId}`);
            check(res, {
              'product detail status is 200': (r) => r.status === 200,
            }) || errorRate.add(1);
          }
          
          sleep(2);
        }
        EOF

    - name: Run performance test
      run: |
        k6 run \
          -e TARGET_URL=${{ env.TARGET_URL }} \
          --out json=test-results.json \
          --out csv=test-results.csv \
          scripts/performance/load-test.js

    - name: Generate HTML report
      run: |
        npm install -g k6-html-reporter
        k6-html-reporter --input test-results.json --output test-report.html

    - name: Analyze results
      run: |
        python << 'EOF'
        import json
        import csv
        import statistics

        # JSON ê²°ê³¼ ë¶„ì„
        with open('test-results.json', 'r') as f:
            results = [json.loads(line) for line in f if line.strip()]
        
        # ë©”íŠ¸ë¦­ ì¶”ì¶œ
        durations = [r['data']['value'] for r in results if r['type'] == 'Point' and r['metric'] == 'http_req_duration']
        errors = [r for r in results if r['type'] == 'Point' and r['metric'] == 'http_req_failed' and r['data']['value'] == 1]
        
        if durations:
            print(f"í‰ê·  ì‘ë‹µ ì‹œê°„: {statistics.mean(durations):.2f}ms")
            print(f"ì¤‘ê°„ê°’ ì‘ë‹µ ì‹œê°„: {statistics.median(durations):.2f}ms")
            print(f"95 ë°±ë¶„ìœ„ìˆ˜: {statistics.quantiles(durations, n=20)[18]:.2f}ms")
            print(f"99 ë°±ë¶„ìœ„ìˆ˜: {statistics.quantiles(durations, n=100)[98]:.2f}ms")
        
        print(f"ì—ëŸ¬ìœ¨: {len(errors) / len(results) * 100:.2f}%")
        
        # ì„ê³„ê°’ ê²€ì‚¬
        p95 = statistics.quantiles(durations, n=20)[18] if durations else 0
        error_rate = len(errors) / len(results) if results else 0
        
        if p95 > 500:
            print("âš ï¸ ê²½ê³ : P95 ì‘ë‹µ ì‹œê°„ì´ 500msë¥¼ ì´ˆê³¼í–ˆìŠµë‹ˆë‹¤!")
        
        if error_rate > 0.1:
            print("âš ï¸ ê²½ê³ : ì—ëŸ¬ìœ¨ì´ 10%ë¥¼ ì´ˆê³¼í–ˆìŠµë‹ˆë‹¤!")
        EOF

    - name: Upload results
      uses: actions/upload-artifact@v3
      with:
        name: performance-test-results
        path: |
          test-results.json
          test-results.csv
          test-report.html

  # ë²¤ì¹˜ë§ˆí¬ í…ŒìŠ¤íŠ¸
  benchmark-test:
    runs-on: ubuntu-latest
    services:
      postgres:
        image: postgres:15
        env:
          POSTGRES_PASSWORD: test123
          POSTGRES_USER: test
          POSTGRES_DB: test_db
        options: >-
          --health-cmd pg_isready
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
      redis:
        image: redis:7-alpine
        options: >-
          --health-cmd "redis-cli ping"
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5

    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.11'

    - name: Install dependencies
      run: |
        cd backend
        pip install -r requirements-dev.txt

    - name: Run benchmark tests
      env:
        DATABASE_URL: postgresql://test:test123@localhost:5432/test_db
        REDIS_URL: redis://localhost:6379/0
        SECRET_KEY: test-secret-key
        USE_V2_SERVICES: true
      run: |
        cd backend
        pytest tests/benchmarks -v --benchmark-only --benchmark-json=benchmark-results.json

    - name: Compare with baseline
      run: |
        cd backend
        if [ -f benchmark-baseline.json ]; then
          pytest-benchmark compare benchmark-baseline.json benchmark-results.json --histogram
        fi

    - name: Store benchmark results
      uses: benchmark-action/github-action-benchmark@v1
      with:
        tool: 'pytest'
        output-file-path: backend/benchmark-results.json
        github-token: ${{ secrets.GITHUB_TOKEN }}
        auto-push: true
        alert-threshold: '200%'
        comment-on-alert: true
        fail-on-alert: true

  # ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§
  synthetic-monitoring:
    runs-on: ubuntu-latest
    if: github.event.schedule || github.event_name == 'workflow_dispatch'
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Install monitoring tools
      run: |
        npm install -g @datadog/synthetics-ci-cli
        pip install newrelic-api

    - name: Run Datadog Synthetics
      env:
        DATADOG_API_KEY: ${{ secrets.DATADOG_API_KEY }}
        DATADOG_APP_KEY: ${{ secrets.DATADOG_APP_KEY }}
      run: |
        datadog-ci synthetics run-tests --config .datadog-ci.json

    - name: Check New Relic metrics
      env:
        NEW_RELIC_API_KEY: ${{ secrets.NEW_RELIC_API_KEY }}
      run: |
        python << 'EOF'
        import requests
        import os
        from datetime import datetime, timedelta

        api_key = os.environ['NEW_RELIC_API_KEY']
        headers = {'Api-Key': api_key}
        
        # ìµœê·¼ 1ì‹œê°„ ë©”íŠ¸ë¦­ ì¡°íšŒ
        end_time = datetime.utcnow()
        start_time = end_time - timedelta(hours=1)
        
        query = """
        {
          actor {
            account(id: 1234567) {
              nrql(query: "SELECT average(duration) FROM Transaction WHERE appName = 'dropshipping-backend' SINCE 1 hour ago") {
                results
              }
            }
          }
        }
        """
        
        response = requests.post(
            'https://api.newrelic.com/graphql',
            headers=headers,
            json={'query': query}
        )
        
        if response.status_code == 200:
            data = response.json()
            avg_duration = data['data']['actor']['account']['nrql']['results'][0]['average.duration']
            print(f"í‰ê·  íŠ¸ëœì­ì…˜ ì‹œê°„: {avg_duration:.2f}ms")
            
            if avg_duration > 500:
                print("âš ï¸ ê²½ê³ : í‰ê·  ì‘ë‹µ ì‹œê°„ì´ 500msë¥¼ ì´ˆê³¼í–ˆìŠµë‹ˆë‹¤!")
        EOF

  # ì„±ëŠ¥ ë¦¬í¬íŠ¸ ìƒì„±
  generate-report:
    runs-on: ubuntu-latest
    needs: [load-test, benchmark-test]
    if: always()
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Download artifacts
      uses: actions/download-artifact@v3

    - name: Generate consolidated report
      run: |
        python << 'EOF'
        import json
        import matplotlib.pyplot as plt
        import pandas as pd
        from datetime import datetime

        # ë¦¬í¬íŠ¸ ìƒì„±
        report = {
            'timestamp': datetime.utcnow().isoformat(),
            'summary': {},
            'recommendations': []
        }

        # ë¶€í•˜ í…ŒìŠ¤íŠ¸ ê²°ê³¼ ë¶„ì„
        try:
            with open('performance-test-results/test-results.json', 'r') as f:
                load_results = [json.loads(line) for line in f if line.strip()]
            
            durations = [r['data']['value'] for r in load_results if r['type'] == 'Point' and r['metric'] == 'http_req_duration']
            if durations:
                report['summary']['load_test'] = {
                    'avg_response_time': sum(durations) / len(durations),
                    'max_response_time': max(durations),
                    'min_response_time': min(durations)
                }
        except:
            pass

        # ê¶Œì¥ì‚¬í•­ ìƒì„±
        if report['summary'].get('load_test', {}).get('avg_response_time', 0) > 300:
            report['recommendations'].append('í‰ê·  ì‘ë‹µ ì‹œê°„ì´ 300msë¥¼ ì´ˆê³¼í•©ë‹ˆë‹¤. ìºì‹± ì „ëµì„ ê²€í† í•˜ì„¸ìš”.')

        # ë¦¬í¬íŠ¸ ì €ì¥
        with open('performance-report.json', 'w') as f:
            json.dump(report, f, indent=2)

        print("ì„±ëŠ¥ ë¦¬í¬íŠ¸ê°€ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤.")
        EOF

    - name: Create issue if performance degrades
      uses: actions/github-script@v6
      if: failure()
      with:
        script: |
          const title = `ğŸš¨ ì„±ëŠ¥ ì €í•˜ ê°ì§€ - ${new Date().toISOString().split('T')[0]}`;
          const body = `## ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ ê²°ê³¼
          
          ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ì—ì„œ ì„ê³„ê°’ì„ ì´ˆê³¼í•˜ëŠ” ì§€í‘œê°€ ë°œê²¬ë˜ì—ˆìŠµë‹ˆë‹¤.
          
          ### ìƒì„¸ ë‚´ìš©
          - í…ŒìŠ¤íŠ¸ ì‹œê°„: ${new Date().toISOString()}
          - ì›Œí¬í”Œë¡œìš°: [${context.workflow}](${context.serverUrl}/${context.repo.owner}/${context.repo.repo}/actions/runs/${context.runId})
          
          ### ì¡°ì¹˜ í•„ìš”
          1. ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ ê²°ê³¼ í™•ì¸
          2. ë³‘ëª© ì§€ì  ë¶„ì„
          3. ìµœì í™” ë°©ì•ˆ ìˆ˜ë¦½
          
          @${context.repo.owner} í™•ì¸ ë¶€íƒë“œë¦½ë‹ˆë‹¤.`;
          
          await github.rest.issues.create({
            owner: context.repo.owner,
            repo: context.repo.repo,
            title: title,
            body: body,
            labels: ['performance', 'bug', 'high-priority']
          });