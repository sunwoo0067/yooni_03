# ë“œëì‰¬í•‘ ìë™í™” ì‹œìŠ¤í…œ ì„±ëŠ¥ ìµœì í™” ê°€ì´ë“œ

## ğŸ“‹ ëª©ì°¨
1. [ì„±ëŠ¥ ìµœì í™” ê°œìš”](#ì„±ëŠ¥-ìµœì í™”-ê°œìš”)
2. [ë°ì´í„°ë² ì´ìŠ¤ ìµœì í™”](#ë°ì´í„°ë² ì´ìŠ¤-ìµœì í™”)
3. [API í˜¸ì¶œ ìµœì í™”](#api-í˜¸ì¶œ-ìµœì í™”)
4. [ë©”ëª¨ë¦¬ ë° CPU ìµœì í™”](#ë©”ëª¨ë¦¬-ë°-cpu-ìµœì í™”)
5. [ë„¤íŠ¸ì›Œí¬ ìµœì í™”](#ë„¤íŠ¸ì›Œí¬-ìµœì í™”)
6. [ìºì‹± ì „ëµ](#ìºì‹±-ì „ëµ)
7. [ë¹„ë™ê¸° ì²˜ë¦¬ ìµœì í™”](#ë¹„ë™ê¸°-ì²˜ë¦¬-ìµœì í™”)
8. [ë¹„ìš© ìµœì í™”](#ë¹„ìš©-ìµœì í™”)
9. [ëª¨ë‹ˆí„°ë§ ë° ì¸¡ì •](#ëª¨ë‹ˆí„°ë§-ë°-ì¸¡ì •)
10. [ì„±ëŠ¥ í…ŒìŠ¤íŠ¸](#ì„±ëŠ¥-í…ŒìŠ¤íŠ¸)

## ğŸš€ ì„±ëŠ¥ ìµœì í™” ê°œìš”

### ìµœì í™” ëª©í‘œ ì„¤ì •
```python
# src/optimization/performance_targets.py
class PerformanceTargets:
    """ì„±ëŠ¥ ëª©í‘œ ì •ì˜"""
    
    TARGETS = {
        # ì‘ë‹µ ì‹œê°„ ëª©í‘œ
        'api_response_time': {
            'excellent': 200,    # 200ms ì´í•˜
            'good': 500,         # 500ms ì´í•˜
            'acceptable': 1000,  # 1ì´ˆ ì´í•˜
            'poor': 2000        # 2ì´ˆ ì´ìƒì€ ê°œì„  í•„ìš”
        },
        
        # ì²˜ë¦¬ëŸ‰ ëª©í‘œ
        'throughput': {
            'product_collection': 1000,  # ì‹œê°„ë‹¹ 1000ê°œ ìƒí’ˆ ìˆ˜ì§‘
            'product_registration': 500, # ì‹œê°„ë‹¹ 500ê°œ ìƒí’ˆ ë“±ë¡
            'order_processing': 100      # ì‹œê°„ë‹¹ 100ê°œ ì£¼ë¬¸ ì²˜ë¦¬
        },
        
        # ë¦¬ì†ŒìŠ¤ ì‚¬ìš©ë¥  ëª©í‘œ
        'resource_usage': {
            'cpu_usage': 70,      # CPU ì‚¬ìš©ë¥  70% ì´í•˜
            'memory_usage': 80,   # ë©”ëª¨ë¦¬ ì‚¬ìš©ë¥  80% ì´í•˜
            'disk_io': 80        # ë””ìŠ¤í¬ I/O 80% ì´í•˜
        },
        
        # ê°€ìš©ì„± ëª©í‘œ
        'availability': {
            'uptime': 99.9,      # 99.9% ê°€ë™ë¥ 
            'error_rate': 0.1    # ì˜¤ë¥˜ìœ¨ 0.1% ì´í•˜
        }
    }
    
    @classmethod
    def get_target(cls, category, metric):
        """ì„±ëŠ¥ ëª©í‘œê°’ ì¡°íšŒ"""
        return cls.TARGETS.get(category, {}).get(metric)
    
    @classmethod
    def evaluate_performance(cls, category, metric, current_value):
        """í˜„ì¬ ì„±ëŠ¥ í‰ê°€"""
        target = cls.get_target(category, metric)
        if not target:
            return "unknown"
        
        if isinstance(target, dict):
            if current_value <= target['excellent']:
                return "excellent"
            elif current_value <= target['good']:
                return "good"
            elif current_value <= target['acceptable']:
                return "acceptable"
            else:
                return "poor"
        else:
            return "good" if current_value <= target else "poor"
```

### ì„±ëŠ¥ ì¸¡ì • ê¸°ì¤€
```python
# src/optimization/performance_metrics.py
import time
import psutil
from functools import wraps

class PerformanceMetrics:
    def __init__(self):
        self.metrics = defaultdict(list)
    
    def measure_execution_time(self, func_name=None):
        """í•¨ìˆ˜ ì‹¤í–‰ ì‹œê°„ ì¸¡ì • ë°ì½”ë ˆì´í„°"""
        def decorator(func):
            @wraps(func)
            async def wrapper(*args, **kwargs):
                start_time = time.time()
                try:
                    result = await func(*args, **kwargs)
                    success = True
                except Exception as e:
                    result = None
                    success = False
                    raise
                finally:
                    end_time = time.time()
                    execution_time = (end_time - start_time) * 1000  # ms
                    
                    self.record_metric(
                        func_name or func.__name__,
                        execution_time,
                        success
                    )
                
                return result
            return wrapper
        return decorator
    
    def record_metric(self, operation, duration, success=True):
        """ì„±ëŠ¥ ë©”íŠ¸ë¦­ ê¸°ë¡"""
        self.metrics[operation].append({
            'timestamp': time.time(),
            'duration': duration,
            'success': success
        })
    
    def get_performance_summary(self, operation, time_window=3600):
        """ì„±ëŠ¥ ìš”ì•½ í†µê³„"""
        current_time = time.time()
        recent_metrics = [
            m for m in self.metrics[operation]
            if current_time - m['timestamp'] <= time_window
        ]
        
        if not recent_metrics:
            return None
        
        durations = [m['duration'] for m in recent_metrics]
        success_count = sum(1 for m in recent_metrics if m['success'])
        
        return {
            'operation': operation,
            'count': len(recent_metrics),
            'success_rate': (success_count / len(recent_metrics)) * 100,
            'avg_duration': sum(durations) / len(durations),
            'min_duration': min(durations),
            'max_duration': max(durations),
            'p95_duration': self.percentile(durations, 95),
            'p99_duration': self.percentile(durations, 99)
        }
    
    def percentile(self, data, percentile):
        """ë°±ë¶„ìœ„ìˆ˜ ê³„ì‚°"""
        sorted_data = sorted(data)
        index = int(len(sorted_data) * percentile / 100)
        return sorted_data[min(index, len(sorted_data) - 1)]
```

## ğŸ—„ï¸ ë°ì´í„°ë² ì´ìŠ¤ ìµœì í™”

### ì¸ë±ìŠ¤ ìµœì í™”
```sql
-- ìƒí’ˆ í…Œì´ë¸” ì¸ë±ìŠ¤ ìµœì í™”
-- products í…Œì´ë¸”
CREATE INDEX CONCURRENTLY idx_products_category_status 
ON products(category, status) 
WHERE status = 'active';

CREATE INDEX CONCURRENTLY idx_products_created_at_desc 
ON products(created_at DESC);

CREATE INDEX CONCURRENTLY idx_products_price_range 
ON products(price) 
WHERE price BETWEEN 1000 AND 1000000;

-- ë³µí•© ì¸ë±ìŠ¤ë¡œ ì¡°íšŒ ì„±ëŠ¥ í–¥ìƒ
CREATE INDEX CONCURRENTLY idx_products_platform_category_status 
ON products(platform, category, status) 
INCLUDE (name, price, stock_quantity);

-- ì£¼ë¬¸ í…Œì´ë¸” ì¸ë±ìŠ¤
CREATE INDEX CONCURRENTLY idx_orders_status_created 
ON orders(status, created_at DESC) 
WHERE status IN ('pending', 'processing');

CREATE INDEX CONCURRENTLY idx_orders_customer_date 
ON orders(customer_id, created_at DESC);

-- íŒŒí‹°ì…”ë‹ì„ ìœ„í•œ ì¸ë±ìŠ¤
CREATE INDEX CONCURRENTLY idx_orders_created_at_month 
ON orders(date_trunc('month', created_at));
```

### ì¿¼ë¦¬ ìµœì í™”
```python
# src/optimization/query_optimizer.py
class QueryOptimizer:
    def __init__(self, db_manager):
        self.db = db_manager
        self.query_cache = {}
    
    async def get_products_optimized(self, filters):
        """ìµœì í™”ëœ ìƒí’ˆ ì¡°íšŒ"""
        # ì¿¼ë¦¬ ìºì‹œ í™•ì¸
        cache_key = self.generate_cache_key(filters)
        if cache_key in self.query_cache:
            return self.query_cache[cache_key]
        
        # ì¸ë±ìŠ¤ë¥¼ í™œìš©í•œ ìµœì í™”ëœ ì¿¼ë¦¬
        query = """
        SELECT p.id, p.name, p.price, p.stock_quantity, p.category
        FROM products p
        WHERE 1=1
        """
        params = []
        
        # ë™ì  WHERE ì ˆ êµ¬ì„± (ì¸ë±ìŠ¤ í™œìš©)
        if filters.get('category'):
            query += " AND p.category = $%d" % (len(params) + 1)
            params.append(filters['category'])
        
        if filters.get('status'):
            query += " AND p.status = $%d" % (len(params) + 1)
            params.append(filters['status'])
        
        if filters.get('price_min'):
            query += " AND p.price >= $%d" % (len(params) + 1)
            params.append(filters['price_min'])
        
        if filters.get('price_max'):
            query += " AND p.price <= $%d" % (len(params) + 1)
            params.append(filters['price_max'])
        
        # ì •ë ¬ ë° ì œí•œ (ì¸ë±ìŠ¤ í™œìš©)
        query += " ORDER BY p.created_at DESC"
        
        if filters.get('limit'):
            query += " LIMIT $%d" % (len(params) + 1)
            params.append(filters['limit'])
        
        result = await self.db.fetch_all(query, *params)
        
        # ê²°ê³¼ ìºì‹± (5ë¶„)
        self.query_cache[cache_key] = result
        asyncio.create_task(self.expire_cache(cache_key, 300))
        
        return result
    
    async def bulk_insert_products(self, products):
        """ëŒ€ëŸ‰ ìƒí’ˆ ì‚½ì… ìµœì í™”"""
        # COPY ëª…ë ¹ì–´ ì‚¬ìš©ìœ¼ë¡œ ì„±ëŠ¥ í–¥ìƒ
        import io
        
        csv_buffer = io.StringIO()
        for product in products:
            csv_buffer.write(f"{product['name']}\t{product['price']}\t{product['category']}\n")
        
        csv_buffer.seek(0)
        
        async with self.db.connection() as conn:
            await conn.copy_from_table(
                'products',
                source=csv_buffer,
                columns=['name', 'price', 'category'],
                format='csv',
                delimiter='\t'
            )
    
    async def analyze_slow_queries(self):
        """ëŠë¦° ì¿¼ë¦¬ ë¶„ì„"""
        query = """
        SELECT 
            query,
            calls,
            total_time,
            mean_time,
            max_time,
            (total_time / calls) as avg_time
        FROM pg_stat_statements
        WHERE calls > 100
        ORDER BY mean_time DESC
        LIMIT 20;
        """
        
        slow_queries = await self.db.fetch_all(query)
        
        recommendations = []
        for q in slow_queries:
            if q['avg_time'] > 1000:  # 1ì´ˆ ì´ìƒ
                recommendations.append({
                    'query': q['query'][:100] + '...',
                    'avg_time': q['avg_time'],
                    'suggestion': self.suggest_optimization(q['query'])
                })
        
        return recommendations
```

### ì»¤ë„¥ì…˜ í’€ ìµœì í™”
```python
# src/optimization/connection_pool_optimizer.py
import asyncpg
from contextlib import asynccontextmanager

class OptimizedConnectionPool:
    def __init__(self, database_url):
        self.database_url = database_url
        self.pool = None
        self.pool_config = {
            'min_size': 5,          # ìµœì†Œ ì—°ê²° ìˆ˜
            'max_size': 20,         # ìµœëŒ€ ì—°ê²° ìˆ˜
            'max_queries': 50000,   # ì—°ê²°ë‹¹ ìµœëŒ€ ì¿¼ë¦¬ ìˆ˜
            'max_inactive_connection_lifetime': 300,  # ë¹„í™œì„± ì—°ê²° ìˆ˜ëª…
            'command_timeout': 60,  # ëª…ë ¹ íƒ€ì„ì•„ì›ƒ
            'server_settings': {
                'application_name': 'dropshipping_system',
                'tcp_keepalives_idle': '600',
                'tcp_keepalives_interval': '30',
                'tcp_keepalives_count': '3',
            }
        }
    
    async def initialize_pool(self):
        """ì»¤ë„¥ì…˜ í’€ ì´ˆê¸°í™”"""
        self.pool = await asyncpg.create_pool(
            self.database_url,
            **self.pool_config
        )
    
    @asynccontextmanager
    async def get_connection(self):
        """ìµœì í™”ëœ ì—°ê²° íšë“"""
        async with self.pool.acquire() as conn:
            # ì—°ê²°ë³„ ìµœì í™” ì„¤ì •
            await conn.execute('SET enable_seqscan = off')  # ì‹œí€€ì…œ ìŠ¤ìº” ë¹„í™œì„±í™”
            await conn.execute('SET random_page_cost = 1.1')  # SSD ìµœì í™”
            await conn.execute('SET effective_cache_size = "4GB"')
            
            yield conn
    
    async def execute_batch_optimized(self, query, data_list):
        """ë°°ì¹˜ ì‹¤í–‰ ìµœì í™”"""
        async with self.get_connection() as conn:
            # íŠ¸ëœì­ì…˜ ë°°ì¹˜ ì²˜ë¦¬
            async with conn.transaction():
                return await conn.executemany(query, data_list)
    
    async def monitor_pool_health(self):
        """ì»¤ë„¥ì…˜ í’€ ìƒíƒœ ëª¨ë‹ˆí„°ë§"""
        if not self.pool:
            return None
        
        return {
            'size': self.pool.get_size(),
            'idle_size': self.pool.get_idle_size(),
            'max_size': self.pool.get_max_size(),
            'min_size': self.pool.get_min_size(),
            'usage_percentage': (self.pool.get_size() - self.pool.get_idle_size()) / self.pool.get_max_size() * 100
        }
```

### íŒŒí‹°ì…”ë‹ ì „ëµ
```sql
-- ì£¼ë¬¸ í…Œì´ë¸” ì›”ë³„ íŒŒí‹°ì…”ë‹
CREATE TABLE orders_master (
    id SERIAL,
    customer_id INT,
    total_amount DECIMAL(10,2),
    status VARCHAR(20),
    created_at TIMESTAMP DEFAULT NOW()
) PARTITION BY RANGE (created_at);

-- ì›”ë³„ íŒŒí‹°ì…˜ ìƒì„±
CREATE TABLE orders_2024_01 PARTITION OF orders_master
FOR VALUES FROM ('2024-01-01') TO ('2024-02-01');

CREATE TABLE orders_2024_02 PARTITION OF orders_master
FOR VALUES FROM ('2024-02-01') TO ('2024-03-01');

-- ìë™ íŒŒí‹°ì…˜ ìƒì„± í•¨ìˆ˜
CREATE OR REPLACE FUNCTION create_monthly_partition(table_name TEXT, start_date DATE)
RETURNS VOID AS $$
DECLARE
    partition_name TEXT;
    end_date DATE;
BEGIN
    partition_name := table_name || '_' || to_char(start_date, 'YYYY_MM');
    end_date := start_date + INTERVAL '1 month';
    
    EXECUTE format(
        'CREATE TABLE %I PARTITION OF %I FOR VALUES FROM (%L) TO (%L)',
        partition_name, table_name, start_date, end_date
    );
    
    -- íŒŒí‹°ì…˜ë³„ ì¸ë±ìŠ¤ ìƒì„±
    EXECUTE format(
        'CREATE INDEX idx_%s_created_at ON %I (created_at)',
        partition_name, partition_name
    );
END;
$$ LANGUAGE plpgsql;
```

## ğŸ”Œ API í˜¸ì¶œ ìµœì í™”

### ë¹„ë™ê¸° HTTP í´ë¼ì´ì–¸íŠ¸ ìµœì í™”
```python
# src/optimization/http_client_optimizer.py
import aiohttp
import asyncio
from aiohttp import ClientTimeout, TCPConnector

class OptimizedHTTPClient:
    def __init__(self):
        self.connector_config = {
            'limit': 100,              # ì´ ì—°ê²° ì œí•œ
            'limit_per_host': 20,      # í˜¸ìŠ¤íŠ¸ë³„ ì—°ê²° ì œí•œ
            'ttl_dns_cache': 300,      # DNS ìºì‹œ TTL
            'use_dns_cache': True,     # DNS ìºì‹± í™œì„±í™”
            'keepalive_timeout': 30,   # Keep-alive íƒ€ì„ì•„ì›ƒ
            'enable_cleanup_closed': True
        }
        
        self.timeout_config = ClientTimeout(
            total=30,      # ì „ì²´ íƒ€ì„ì•„ì›ƒ
            connect=10,    # ì—°ê²° íƒ€ì„ì•„ì›ƒ
            sock_read=20   # ì½ê¸° íƒ€ì„ì•„ì›ƒ
        )
        
        self.session = None
    
    async def __aenter__(self):
        connector = TCPConnector(**self.connector_config)
        self.session = aiohttp.ClientSession(
            connector=connector,
            timeout=self.timeout_config,
            headers={
                'User-Agent': 'DropshippingBot/1.0',
                'Accept-Encoding': 'gzip, deflate',
                'Connection': 'keep-alive'
            }
        )
        return self
    
    async def __aexit__(self, exc_type, exc_val, exc_tb):
        if self.session:
            await self.session.close()
    
    async def fetch_with_retry(self, url, max_retries=3, **kwargs):
        """ì¬ì‹œë„ ë¡œì§ì´ í¬í•¨ëœ HTTP ìš”ì²­"""
        for attempt in range(max_retries):
            try:
                async with self.session.get(url, **kwargs) as response:
                    if response.status == 200:
                        return await response.json()
                    elif response.status == 429:  # Rate limit
                        await asyncio.sleep(2 ** attempt)  # ì§€ìˆ˜ ë°±ì˜¤í”„
                        continue
                    else:
                        response.raise_for_status()
            except (aiohttp.ClientError, asyncio.TimeoutError) as e:
                if attempt == max_retries - 1:
                    raise
                await asyncio.sleep(1 * attempt)
        
        return None
    
    async def fetch_batch_optimized(self, urls, concurrency_limit=10):
        """ë°°ì¹˜ ìš”ì²­ ìµœì í™”"""
        semaphore = asyncio.Semaphore(concurrency_limit)
        
        async def fetch_single(url):
            async with semaphore:
                return await self.fetch_with_retry(url)
        
        tasks = [fetch_single(url) for url in urls]
        return await asyncio.gather(*tasks, return_exceptions=True)
```

### API ë ˆì´íŠ¸ ë¦¬ë°‹ ê´€ë¦¬
```python
# src/optimization/rate_limiter.py
import asyncio
import time
from collections import defaultdict

class RateLimiter:
    def __init__(self):
        self.limits = {
            'gentrade': {'calls': 1000, 'period': 3600},   # ì‹œê°„ë‹¹ 1000íšŒ
            'ownersclan': {'calls': 500, 'period': 3600},  # ì‹œê°„ë‹¹ 500íšŒ
            'coupang': {'calls': 100, 'period': 60},       # ë¶„ë‹¹ 100íšŒ
            'naver': {'calls': 200, 'period': 60}          # ë¶„ë‹¹ 200íšŒ
        }
        self.call_history = defaultdict(list)
        self.locks = defaultdict(asyncio.Lock)
    
    async def acquire(self, service_name):
        """API í˜¸ì¶œ ê¶Œí•œ íšë“"""
        async with self.locks[service_name]:
            limit_config = self.limits.get(service_name)
            if not limit_config:
                return True
            
            current_time = time.time()
            history = self.call_history[service_name]
            
            # ë§Œë£Œëœ í˜¸ì¶œ ê¸°ë¡ ì œê±°
            cutoff_time = current_time - limit_config['period']
            self.call_history[service_name] = [
                call_time for call_time in history 
                if call_time > cutoff_time
            ]
            
            # ì œí•œ í™•ì¸
            if len(self.call_history[service_name]) >= limit_config['calls']:
                # ë‹¤ìŒ í˜¸ì¶œ ê°€ëŠ¥ ì‹œê°„ê¹Œì§€ ëŒ€ê¸°
                oldest_call = min(self.call_history[service_name])
                wait_time = oldest_call + limit_config['period'] - current_time
                
                if wait_time > 0:
                    await asyncio.sleep(wait_time)
            
            # í˜¸ì¶œ ê¸°ë¡ ì¶”ê°€
            self.call_history[service_name].append(current_time)
            return True
    
    async def get_remaining_calls(self, service_name):
        """ë‚¨ì€ í˜¸ì¶œ ìˆ˜ í™•ì¸"""
        limit_config = self.limits.get(service_name)
        if not limit_config:
            return float('inf')
        
        current_time = time.time()
        cutoff_time = current_time - limit_config['period']
        
        recent_calls = [
            call_time for call_time in self.call_history[service_name]
            if call_time > cutoff_time
        ]
        
        return max(0, limit_config['calls'] - len(recent_calls))
```

### ì‘ë‹µ ìºì‹± ì „ëµ
```python
# src/optimization/response_cache.py
import hashlib
import json
import asyncio
from datetime import datetime, timedelta

class ResponseCache:
    def __init__(self, redis_client):
        self.redis = redis_client
        self.cache_config = {
            'product_info': {'ttl': 3600, 'compress': True},      # 1ì‹œê°„
            'market_analysis': {'ttl': 21600, 'compress': True},  # 6ì‹œê°„
            'competitor_data': {'ttl': 7200, 'compress': True},   # 2ì‹œê°„
            'image_processing': {'ttl': 86400, 'compress': False} # 24ì‹œê°„
        }
    
    def generate_cache_key(self, endpoint, params):
        """ìºì‹œ í‚¤ ìƒì„±"""
        param_str = json.dumps(params, sort_keys=True)
        key_data = f"{endpoint}:{param_str}"
        return hashlib.md5(key_data.encode()).hexdigest()
    
    async def get_cached_response(self, cache_type, key):
        """ìºì‹œëœ ì‘ë‹µ ì¡°íšŒ"""
        cache_key = f"{cache_type}:{key}"
        cached_data = await self.redis.get(cache_key)
        
        if cached_data:
            config = self.cache_config.get(cache_type, {})
            if config.get('compress'):
                cached_data = self.decompress(cached_data)
            
            return json.loads(cached_data)
        
        return None
    
    async def cache_response(self, cache_type, key, data):
        """ì‘ë‹µ ìºì‹±"""
        config = self.cache_config.get(cache_type, {})
        ttl = config.get('ttl', 3600)
        
        cached_data = json.dumps(data)
        if config.get('compress'):
            cached_data = self.compress(cached_data)
        
        cache_key = f"{cache_type}:{key}"
        await self.redis.setex(cache_key, ttl, cached_data)
    
    async def invalidate_cache(self, cache_type, pattern=None):
        """ìºì‹œ ë¬´íš¨í™”"""
        if pattern:
            keys = await self.redis.keys(f"{cache_type}:{pattern}")
        else:
            keys = await self.redis.keys(f"{cache_type}:*")
        
        if keys:
            await self.redis.delete(*keys)
    
    def compress(self, data):
        """ë°ì´í„° ì••ì¶•"""
        import gzip
        return gzip.compress(data.encode())
    
    def decompress(self, data):
        """ë°ì´í„° ì••ì¶• í•´ì œ"""
        import gzip
        return gzip.decompress(data).decode()
```

## ğŸ’¾ ë©”ëª¨ë¦¬ ë° CPU ìµœì í™”

### ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ìµœì í™”
```python
# src/optimization/memory_optimizer.py
import gc
import sys
import psutil
from memory_profiler import profile

class MemoryOptimizer:
    def __init__(self):
        self.process = psutil.Process()
        self.memory_threshold = 1024 * 1024 * 1024  # 1GB
    
    def monitor_memory_usage(self):
        """ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ëª¨ë‹ˆí„°ë§"""
        memory_info = self.process.memory_info()
        
        return {
            'rss': memory_info.rss,           # ì‹¤ì œ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰
            'vms': memory_info.vms,           # ê°€ìƒ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰
            'percent': self.process.memory_percent(),
            'available': psutil.virtual_memory().available
        }
    
    async def optimize_large_dataset_processing(self, dataset):
        """ëŒ€ìš©ëŸ‰ ë°ì´í„°ì…‹ ì²˜ë¦¬ ìµœì í™”"""
        # ì²­í¬ ë‹¨ìœ„ë¡œ ì²˜ë¦¬í•˜ì—¬ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì œí•œ
        chunk_size = 1000
        results = []
        
        for i in range(0, len(dataset), chunk_size):
            chunk = dataset[i:i + chunk_size]
            
            # ì²­í¬ ì²˜ë¦¬
            chunk_result = await self.process_chunk(chunk)
            results.extend(chunk_result)
            
            # ë©”ëª¨ë¦¬ ì •ë¦¬
            del chunk
            del chunk_result
            
            # ê°€ë¹„ì§€ ì»¬ë ‰ì…˜ ê°•ì œ ì‹¤í–‰
            if i % (chunk_size * 10) == 0:
                gc.collect()
            
            # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì²´í¬
            if self.process.memory_info().rss > self.memory_threshold:
                gc.collect()
                await asyncio.sleep(0.1)  # CPU ë¶€í•˜ ë¶„ì‚°
        
        return results
    
    def optimize_object_creation(self):
        """ê°ì²´ ìƒì„± ìµœì í™”"""
        # __slots__ ì‚¬ìš©ìœ¼ë¡œ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ê°ì†Œ
        class OptimizedProduct:
            __slots__ = ['id', 'name', 'price', 'category', 'stock']
            
            def __init__(self, id, name, price, category, stock):
                self.id = id
                self.name = name
                self.price = price
                self.category = category
                self.stock = stock
        
        return OptimizedProduct
    
    async def implement_lazy_loading(self, product_ids):
        """ì§€ì—° ë¡œë”© êµ¬í˜„"""
        class LazyProductLoader:
            def __init__(self, product_ids):
                self.product_ids = product_ids
                self._cache = {}
            
            async def get_product(self, product_id):
                if product_id not in self._cache:
                    self._cache[product_id] = await self.load_product(product_id)
                return self._cache[product_id]
            
            async def load_product(self, product_id):
                # ì‹¤ì œ ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ ë¡œë“œ
                return await database.fetch_product(product_id)
        
        return LazyProductLoader(product_ids)
```

### CPU ì‚¬ìš©ëŸ‰ ìµœì í™”
```python
# src/optimization/cpu_optimizer.py
import asyncio
import multiprocessing
from concurrent.futures import ProcessPoolExecutor, ThreadPoolExecutor

class CPUOptimizer:
    def __init__(self):
        self.cpu_count = multiprocessing.cpu_count()
        self.process_pool = ProcessPoolExecutor(max_workers=self.cpu_count)
        self.thread_pool = ThreadPoolExecutor(max_workers=self.cpu_count * 2)
    
    async def cpu_intensive_task_async(self, data_chunks):
        """CPU ì§‘ì•½ì  ì‘ì—… ë¹„ë™ê¸° ì²˜ë¦¬"""
        # í”„ë¡œì„¸ìŠ¤ í’€ì„ ì‚¬ìš©í•œ ë³‘ë ¬ ì²˜ë¦¬
        loop = asyncio.get_event_loop()
        
        tasks = []
        for chunk in data_chunks:
            task = loop.run_in_executor(
                self.process_pool,
                self.process_chunk_cpu_intensive,
                chunk
            )
            tasks.append(task)
        
        results = await asyncio.gather(*tasks)
        return results
    
    def process_chunk_cpu_intensive(self, chunk):
        """CPU ì§‘ì•½ì  ì²˜ë¦¬ (ë³„ë„ í”„ë¡œì„¸ìŠ¤ì—ì„œ ì‹¤í–‰)"""
        # ì˜ˆ: ë³µì¡í•œ ê³„ì‚°, ì´ë¯¸ì§€ ì²˜ë¦¬, ë°ì´í„° ë¶„ì„
        import numpy as np
        
        # NumPyë¥¼ ì‚¬ìš©í•œ ë²¡í„°í™” ì—°ì‚°ìœ¼ë¡œ ì„±ëŠ¥ í–¥ìƒ
        data_array = np.array(chunk)
        
        # ë²¡í„°í™”ëœ ì—°ì‚°
        result = np.mean(data_array) * np.std(data_array)
        
        return result
    
    async def io_bound_task_async(self, urls):
        """I/O ë°”ìš´ë“œ ì‘ì—… ìµœì í™”"""
        # ìŠ¤ë ˆë“œ í’€ì„ ì‚¬ìš©í•œ I/O ì‘ì—…
        loop = asyncio.get_event_loop()
        
        tasks = []
        for url in urls:
            task = loop.run_in_executor(
                self.thread_pool,
                self.fetch_url_sync,
                url
            )
            tasks.append(task)
        
        results = await asyncio.gather(*tasks, return_exceptions=True)
        return results
    
    def optimize_loops(self, data):
        """ë£¨í”„ ìµœì í™”"""
        # ë¦¬ìŠ¤íŠ¸ ì»´í”„ë¦¬í—¨ì…˜ ì‚¬ìš©
        # ê¸°ì¡´: 
        # result = []
        # for item in data:
        #     if item > 10:
        #         result.append(item * 2)
        
        # ìµœì í™”:
        result = [item * 2 for item in data if item > 10]
        
        # NumPy ë²¡í„°í™” ì—°ì‚° ì‚¬ìš©
        import numpy as np
        data_array = np.array(data)
        result_vectorized = data_array[data_array > 10] * 2
        
        return result_vectorized
    
    async def batch_processing_optimization(self, items, batch_size=100):
        """ë°°ì¹˜ ì²˜ë¦¬ ìµœì í™”"""
        semaphore = asyncio.Semaphore(10)  # ë™ì‹œ ì‹¤í–‰ ì œí•œ
        
        async def process_batch(batch):
            async with semaphore:
                return await self.process_items_batch(batch)
        
        # ë°°ì¹˜ ë‹¨ìœ„ë¡œ ë¶„í• 
        batches = [
            items[i:i + batch_size] 
            for i in range(0, len(items), batch_size)
        ]
        
        # ë°°ì¹˜ ë³‘ë ¬ ì²˜ë¦¬
        tasks = [process_batch(batch) for batch in batches]
        results = await asyncio.gather(*tasks)
        
        # ê²°ê³¼ í‰íƒ„í™”
        return [item for batch_result in results for item in batch_result]
```

## ğŸŒ ë„¤íŠ¸ì›Œí¬ ìµœì í™”

### ì—°ê²° í’€ë§ ìµœì í™”
```python
# src/optimization/connection_pooling.py
import aiohttp
import asyncio
from aiohttp import TCPConnector, ClientTimeout

class NetworkOptimizer:
    def __init__(self):
        self.connectors = {}
        self.sessions = {}
    
    def create_optimized_connector(self, service_name):
        """ì„œë¹„ìŠ¤ë³„ ìµœì í™”ëœ ì»¤ë„¥í„° ìƒì„±"""
        connector_configs = {
            'default': {
                'limit': 100,
                'limit_per_host': 20,
                'ttl_dns_cache': 300,
                'use_dns_cache': True,
                'keepalive_timeout': 30
            },
            'high_volume': {
                'limit': 500,
                'limit_per_host': 50,
                'ttl_dns_cache': 600,
                'use_dns_cache': True,
                'keepalive_timeout': 60
            },
            'external_api': {
                'limit': 50,
                'limit_per_host': 10,
                'ttl_dns_cache': 120,
                'use_dns_cache': True,
                'keepalive_timeout': 20
            }
        }
        
        config = connector_configs.get(service_name, connector_configs['default'])
        return TCPConnector(**config)
    
    async def create_optimized_session(self, service_name):
        """ìµœì í™”ëœ HTTP ì„¸ì…˜ ìƒì„±"""
        if service_name not in self.sessions:
            connector = self.create_optimized_connector(service_name)
            
            timeout = ClientTimeout(
                total=30,
                connect=10,
                sock_read=20
            )
            
            session = aiohttp.ClientSession(
                connector=connector,
                timeout=timeout,
                headers={
                    'User-Agent': 'DropshippingBot/1.0',
                    'Accept-Encoding': 'gzip, deflate, br',
                    'Connection': 'keep-alive',
                    'Accept': 'application/json'
                }
            )
            
            self.sessions[service_name] = session
        
        return self.sessions[service_name]
    
    async def optimize_dns_resolution(self):
        """DNS í•´ì„ ìµœì í™”"""
        # ìì£¼ ì‚¬ìš©í•˜ëŠ” ë„ë©”ì¸ ì‚¬ì „ í•´ì„
        import socket
        
        domains = [
            'api.gentrade.co.kr',
            'api.ownersclan.com',
            'api.domemegguk.com',
            'api.coupang.com',
            'commerce.naver.com'
        ]
        
        tasks = []
        for domain in domains:
            task = asyncio.create_task(self.resolve_domain(domain))
            tasks.append(task)
        
        await asyncio.gather(*tasks, return_exceptions=True)
    
    async def resolve_domain(self, domain):
        """ë„ë©”ì¸ í•´ì„"""
        loop = asyncio.get_event_loop()
        try:
            return await loop.getaddrinfo(domain, None)
        except Exception:
            return None
```

### ëŒ€ì—­í­ ìµœì í™”
```python
# src/optimization/bandwidth_optimizer.py
class BandwidthOptimizer:
    def __init__(self):
        self.compression_enabled = True
        self.image_quality_settings = {
            'thumbnail': 60,    # 60% í’ˆì§ˆ
            'medium': 80,       # 80% í’ˆì§ˆ
            'high': 95          # 95% í’ˆì§ˆ
        }
    
    async def optimize_image_transfer(self, image_url, quality='medium'):
        """ì´ë¯¸ì§€ ì „ì†¡ ìµœì í™”"""
        import aiohttp
        from PIL import Image
        import io
        
        async with aiohttp.ClientSession() as session:
            async with session.get(image_url) as response:
                image_data = await response.read()
        
        # ì´ë¯¸ì§€ ì••ì¶•
        image = Image.open(io.BytesIO(image_data))
        
        # WebP í˜•ì‹ìœ¼ë¡œ ë³€í™˜ (ë” ë‚˜ì€ ì••ì¶•ë¥ )
        output_buffer = io.BytesIO()
        quality_setting = self.image_quality_settings.get(quality, 80)
        
        image.save(
            output_buffer, 
            format='WebP', 
            quality=quality_setting,
            optimize=True
        )
        
        compressed_data = output_buffer.getvalue()
        compression_ratio = len(compressed_data) / len(image_data)
        
        return {
            'data': compressed_data,
            'original_size': len(image_data),
            'compressed_size': len(compressed_data),
            'compression_ratio': compression_ratio
        }
    
    async def implement_progressive_loading(self, large_dataset):
        """ì ì§„ì  ë¡œë”© êµ¬í˜„"""
        page_size = 50
        total_pages = (len(large_dataset) + page_size - 1) // page_size
        
        for page in range(total_pages):
            start_idx = page * page_size
            end_idx = min(start_idx + page_size, len(large_dataset))
            
            page_data = large_dataset[start_idx:end_idx]
            
            yield {
                'page': page + 1,
                'total_pages': total_pages,
                'data': page_data,
                'has_more': page < total_pages - 1
            }
    
    async def optimize_api_payload(self, data):
        """API í˜ì´ë¡œë“œ ìµœì í™”"""
        import json
        import gzip
        
        # JSON ì••ì¶•
        json_data = json.dumps(data, separators=(',', ':'))  # ê³µë°± ì œê±°
        
        # gzip ì••ì¶•
        compressed_data = gzip.compress(json_data.encode())
        
        return {
            'data': compressed_data,
            'headers': {
                'Content-Encoding': 'gzip',
                'Content-Type': 'application/json'
            },
            'compression_ratio': len(compressed_data) / len(json_data)
        }
```

## ğŸš€ ìºì‹± ì „ëµ

### ë‹¤ì¸µ ìºì‹± ì‹œìŠ¤í…œ
```python
# src/optimization/multi_layer_cache.py
import asyncio
import json
from datetime import datetime, timedelta

class MultiLayerCache:
    def __init__(self, redis_client, local_cache_size=1000):
        self.redis = redis_client
        self.local_cache = {}
        self.local_cache_size = local_cache_size
        self.access_times = {}
        
        # ìºì‹œ ë ˆë²¨ë³„ TTL ì„¤ì •
        self.cache_levels = {
            'l1_memory': {'ttl': 300, 'max_size': 1000},      # 5ë¶„, ë©”ëª¨ë¦¬
            'l2_redis': {'ttl': 3600, 'max_size': 10000},     # 1ì‹œê°„, Redis
            'l3_database': {'ttl': 86400, 'max_size': 100000} # 24ì‹œê°„, ë°ì´í„°ë² ì´ìŠ¤
        }
    
    async def get(self, key, fallback_func=None):
        """ë‹¤ì¸µ ìºì‹œì—ì„œ ë°ì´í„° ì¡°íšŒ"""
        # L1: ë©”ëª¨ë¦¬ ìºì‹œ í™•ì¸
        if key in self.local_cache:
            self.access_times[key] = datetime.now()
            return self.local_cache[key]['data']
        
        # L2: Redis ìºì‹œ í™•ì¸
        redis_key = f"cache:l2:{key}"
        redis_data = await self.redis.get(redis_key)
        
        if redis_data:
            data = json.loads(redis_data)
            # L1 ìºì‹œì— ì €ì¥
            await self.set_local_cache(key, data)
            return data
        
        # L3: ë°ì´í„°ë² ì´ìŠ¤ ë˜ëŠ” ì™¸ë¶€ API
        if fallback_func:
            data = await fallback_func()
            if data is not None:
                await self.set(key, data)
                return data
        
        return None
    
    async def set(self, key, data, ttl=None):
        """ë‹¤ì¸µ ìºì‹œì— ë°ì´í„° ì €ì¥"""
        # L1: ë©”ëª¨ë¦¬ ìºì‹œ
        await self.set_local_cache(key, data)
        
        # L2: Redis ìºì‹œ
        redis_key = f"cache:l2:{key}"
        redis_ttl = ttl or self.cache_levels['l2_redis']['ttl']
        await self.redis.setex(
            redis_key, 
            redis_ttl, 
            json.dumps(data)
        )
    
    async def set_local_cache(self, key, data):
        """ë¡œì»¬ ë©”ëª¨ë¦¬ ìºì‹œ ì„¤ì •"""
        # ìºì‹œ í¬ê¸° ì œí•œ
        if len(self.local_cache) >= self.local_cache_size:
            await self.evict_lru()
        
        self.local_cache[key] = {
            'data': data,
            'timestamp': datetime.now()
        }
        self.access_times[key] = datetime.now()
    
    async def evict_lru(self):
        """LRU ë°©ì‹ìœ¼ë¡œ ìºì‹œ ì œê±°"""
        if not self.access_times:
            return
        
        # ê°€ì¥ ì˜¤ë˜ ì „ì— ì ‘ê·¼í•œ í•­ëª© ì°¾ê¸°
        oldest_key = min(self.access_times.items(), key=lambda x: x[1])[0]
        
        # ì œê±°
        del self.local_cache[oldest_key]
        del self.access_times[oldest_key]
    
    async def invalidate(self, key):
        """ìºì‹œ ë¬´íš¨í™”"""
        # L1 ìºì‹œ ì œê±°
        if key in self.local_cache:
            del self.local_cache[key]
        if key in self.access_times:
            del self.access_times[key]
        
        # L2 ìºì‹œ ì œê±°
        redis_key = f"cache:l2:{key}"
        await self.redis.delete(redis_key)
    
    async def warm_up_cache(self, frequently_accessed_keys):
        """ìºì‹œ ì›Œë°ì—…"""
        tasks = []
        for key, fallback_func in frequently_accessed_keys.items():
            task = asyncio.create_task(self.get(key, fallback_func))
            tasks.append(task)
        
        await asyncio.gather(*tasks, return_exceptions=True)
```

### ìºì‹œ ë¬´íš¨í™” ì „ëµ
```python
# src/optimization/cache_invalidation.py
class CacheInvalidationManager:
    def __init__(self, cache_manager):
        self.cache = cache_manager
        self.dependency_graph = {}
        self.tag_mappings = {}
    
    def register_dependencies(self, key, dependencies):
        """ìºì‹œ ì˜ì¡´ì„± ë“±ë¡"""
        self.dependency_graph[key] = dependencies
    
    def add_cache_tags(self, key, tags):
        """ìºì‹œ íƒœê·¸ ì¶”ê°€"""
        for tag in tags:
            if tag not in self.tag_mappings:
                self.tag_mappings[tag] = set()
            self.tag_mappings[tag].add(key)
    
    async def invalidate_by_key(self, key):
        """í‚¤ ê¸°ë°˜ ìºì‹œ ë¬´íš¨í™”"""
        # ì§ì ‘ ë¬´íš¨í™”
        await self.cache.invalidate(key)
        
        # ì˜ì¡´ì„± ê¸°ë°˜ ë¬´íš¨í™”
        if key in self.dependency_graph:
            for dependent_key in self.dependency_graph[key]:
                await self.cache.invalidate(dependent_key)
    
    async def invalidate_by_tag(self, tag):
        """íƒœê·¸ ê¸°ë°˜ ìºì‹œ ë¬´íš¨í™”"""
        if tag in self.tag_mappings:
            for key in self.tag_mappings[tag]:
                await self.cache.invalidate(key)
            
            # íƒœê·¸ ë§¤í•‘ ì •ë¦¬
            del self.tag_mappings[tag]
    
    async def invalidate_pattern(self, pattern):
        """íŒ¨í„´ ê¸°ë°˜ ìºì‹œ ë¬´íš¨í™”"""
        import re
        
        # Redisì—ì„œ íŒ¨í„´ ë§¤ì¹­ í‚¤ ì°¾ê¸°
        redis_keys = await self.cache.redis.keys(f"cache:*{pattern}*")
        
        for redis_key in redis_keys:
            # ì‹¤ì œ ìºì‹œ í‚¤ ì¶”ì¶œ
            cache_key = redis_key.replace("cache:l2:", "")
            await self.cache.invalidate(cache_key)
    
    async def time_based_invalidation(self):
        """ì‹œê°„ ê¸°ë°˜ ìºì‹œ ë¬´íš¨í™”"""
        current_time = datetime.now()
        
        # ë¡œì»¬ ìºì‹œ TTL ì²´í¬
        expired_keys = []
        for key, cache_item in self.cache.local_cache.items():
            ttl = self.cache.cache_levels['l1_memory']['ttl']
            if (current_time - cache_item['timestamp']).seconds > ttl:
                expired_keys.append(key)
        
        # ë§Œë£Œëœ í‚¤ ì œê±°
        for key in expired_keys:
            await self.cache.invalidate(key)
```

## âš¡ ë¹„ë™ê¸° ì²˜ë¦¬ ìµœì í™”

### í ì‹œìŠ¤í…œ ìµœì í™”
```python
# src/optimization/queue_optimizer.py
import asyncio
import aioredis
from enum import Enum

class Priority(Enum):
    HIGH = 1
    MEDIUM = 2
    LOW = 3

class OptimizedQueue:
    def __init__(self, redis_url):
        self.redis = aioredis.from_url(redis_url)
        self.queue_names = {
            Priority.HIGH: 'queue:high',
            Priority.MEDIUM: 'queue:medium',
            Priority.LOW: 'queue:low'
        }
        self.workers = []
        self.max_workers = 10
        self.batch_size = 50
    
    async def enqueue(self, task_data, priority=Priority.MEDIUM):
        """ìš°ì„ ìˆœìœ„ ê¸°ë°˜ ì‘ì—… íì‰"""
        queue_name = self.queue_names[priority]
        
        task_json = json.dumps({
            'id': str(uuid.uuid4()),
            'data': task_data,
            'timestamp': time.time(),
            'priority': priority.value
        })
        
        await self.redis.lpush(queue_name, task_json)
    
    async def start_workers(self):
        """ì›Œì»¤ ì‹œì‘"""
        for i in range(self.max_workers):
            worker = asyncio.create_task(self.worker_loop(f"worker-{i}"))
            self.workers.append(worker)
    
    async def worker_loop(self, worker_name):
        """ì›Œì»¤ ë©”ì¸ ë£¨í”„"""
        while True:
            try:
                # ìš°ì„ ìˆœìœ„ ìˆœì„œë¡œ í ì²´í¬
                task = await self.get_next_task()
                
                if task:
                    await self.process_task(task, worker_name)
                else:
                    # ì‘ì—…ì´ ì—†ìœ¼ë©´ ì ì‹œ ëŒ€ê¸°
                    await asyncio.sleep(1)
                    
            except Exception as e:
                logger.error(f"Worker {worker_name} error: {str(e)}")
                await asyncio.sleep(5)
    
    async def get_next_task(self):
        """ë‹¤ìŒ ì‘ì—… ì¡°íšŒ (ìš°ì„ ìˆœìœ„ ìˆœ)"""
        for priority in Priority:
            queue_name = self.queue_names[priority]
            task_json = await self.redis.rpop(queue_name)
            
            if task_json:
                return json.loads(task_json)
        
        return None
    
    async def process_task(self, task, worker_name):
        """ì‘ì—… ì²˜ë¦¬"""
        start_time = time.time()
        
        try:
            # ì‘ì—… ì‹¤í–‰
            result = await self.execute_task(task['data'])
            
            # ì„±ê³µ ë¡œê¹…
            execution_time = time.time() - start_time
            logger.info(f"Task {task['id']} completed by {worker_name} in {execution_time:.2f}s")
            
        except Exception as e:
            # ì‹¤íŒ¨ ì²˜ë¦¬
            await self.handle_task_failure(task, str(e))
            logger.error(f"Task {task['id']} failed: {str(e)}")
    
    async def batch_process_optimization(self):
        """ë°°ì¹˜ ì²˜ë¦¬ ìµœì í™”"""
        while True:
            # ê° ìš°ì„ ìˆœìœ„ë³„ë¡œ ë°°ì¹˜ ìˆ˜ì§‘
            batches = {}
            
            for priority in Priority:
                queue_name = self.queue_names[priority]
                batch = []
                
                for _ in range(self.batch_size):
                    task_json = await self.redis.rpop(queue_name)
                    if task_json:
                        batch.append(json.loads(task_json))
                    else:
                        break
                
                if batch:
                    batches[priority] = batch
            
            # ë°°ì¹˜ ì²˜ë¦¬
            for priority, batch in batches.items():
                await self.process_batch(batch, priority)
            
            if not batches:
                await asyncio.sleep(5)
    
    async def process_batch(self, batch, priority):
        """ë°°ì¹˜ ì‘ì—… ì²˜ë¦¬"""
        tasks = []
        for item in batch:
            task = asyncio.create_task(self.execute_task(item['data']))
            tasks.append(task)
        
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        # ê²°ê³¼ ì²˜ë¦¬
        for i, result in enumerate(results):
            if isinstance(result, Exception):
                await self.handle_task_failure(batch[i], str(result))
```

### ë™ì‹œì„± ì œì–´ ìµœì í™”
```python
# src/optimization/concurrency_optimizer.py
import asyncio
from asyncio import Semaphore, Lock
from collections import defaultdict

class ConcurrencyOptimizer:
    def __init__(self):
        self.semaphores = {}
        self.locks = defaultdict(Lock)
        self.rate_limiters = {}
    
    def create_semaphore(self, name, limit):
        """ì„¸ë§ˆí¬ì–´ ìƒì„±"""
        self.semaphores[name] = Semaphore(limit)
        return self.semaphores[name]
    
    async def controlled_execution(self, semaphore_name, coro):
        """ì œì–´ëœ ì‹¤í–‰"""
        if semaphore_name not in self.semaphores:
            raise ValueError(f"Semaphore {semaphore_name} not found")
        
        semaphore = self.semaphores[semaphore_name]
        
        async with semaphore:
            return await coro
    
    async def resource_aware_processing(self, tasks, max_memory_mb=1000):
        """ë¦¬ì†ŒìŠ¤ ì¸ì‹ ì²˜ë¦¬"""
        import psutil
        
        active_tasks = []
        completed_results = []
        
        for task in tasks:
            # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì²´í¬
            memory_usage = psutil.virtual_memory().percent
            
            if memory_usage > 85:  # 85% ì´ìƒì´ë©´ ëŒ€ê¸°
                # ê¸°ì¡´ ì‘ì—… ì™„ë£Œ ëŒ€ê¸°
                if active_tasks:
                    done, active_tasks = await asyncio.wait(
                        active_tasks, 
                        return_when=asyncio.FIRST_COMPLETED
                    )
                    completed_results.extend([t.result() for t in done])
            
            # ìƒˆ ì‘ì—… ì‹œì‘
            active_task = asyncio.create_task(task)
            active_tasks.add(active_task)
            
            # ë™ì‹œ ì‹¤í–‰ ì œí•œ
            if len(active_tasks) >= 20:
                done, active_tasks = await asyncio.wait(
                    active_tasks,
                    return_when=asyncio.FIRST_COMPLETED
                )
                completed_results.extend([t.result() for t in done])
        
        # ë‚¨ì€ ì‘ì—… ì™„ë£Œ
        if active_tasks:
            done = await asyncio.gather(*active_tasks)
            completed_results.extend(done)
        
        return completed_results
    
    async def adaptive_concurrency(self, task_generator, initial_concurrency=10):
        """ì ì‘í˜• ë™ì‹œì„± ì œì–´"""
        current_concurrency = initial_concurrency
        performance_history = []
        
        while True:
            try:
                # í˜„ì¬ ë™ì‹œì„± ìˆ˜ì¤€ìœ¼ë¡œ ì‘ì—… ì‹¤í–‰
                tasks = [next(task_generator) for _ in range(current_concurrency)]
                
                start_time = time.time()
                results = await asyncio.gather(*tasks, return_exceptions=True)
                execution_time = time.time() - start_time
                
                # ì„±ëŠ¥ ë©”íŠ¸ë¦­ ê³„ì‚°
                success_count = sum(1 for r in results if not isinstance(r, Exception))
                throughput = success_count / execution_time
                
                performance_history.append({
                    'concurrency': current_concurrency,
                    'throughput': throughput,
                    'success_rate': success_count / len(results)
                })
                
                # ë™ì‹œì„± ì¡°ì •
                current_concurrency = self.adjust_concurrency(
                    performance_history, 
                    current_concurrency
                )
                
            except StopIteration:
                break
            except Exception as e:
                logger.error(f"Adaptive concurrency error: {str(e)}")
                current_concurrency = max(1, current_concurrency // 2)
    
    def adjust_concurrency(self, history, current):
        """ë™ì‹œì„± ìˆ˜ì¤€ ì¡°ì •"""
        if len(history) < 3:
            return current
        
        recent = history[-3:]
        avg_throughput = sum(h['throughput'] for h in recent) / len(recent)
        avg_success_rate = sum(h['success_rate'] for h in recent) / len(recent)
        
        # ì„±ê³µë¥ ì´ 90% ì´ìƒì´ê³  ì²˜ë¦¬ëŸ‰ì´ ì¦ê°€í•˜ë©´ ë™ì‹œì„± ì¦ê°€
        if avg_success_rate > 0.9 and recent[-1]['throughput'] > recent[-2]['throughput']:
            return min(current + 5, 100)
        
        # ì„±ê³µë¥ ì´ 80% ë¯¸ë§Œì´ë©´ ë™ì‹œì„± ê°ì†Œ
        elif avg_success_rate < 0.8:
            return max(current - 5, 1)
        
        return current
```

## ğŸ“ˆ ëª¨ë‹ˆí„°ë§ ë° ì¸¡ì •

### ì„±ëŠ¥ ë©”íŠ¸ë¦­ ìˆ˜ì§‘
```python
# src/optimization/metrics_collector.py
import time
import asyncio
import psutil
from dataclasses import dataclass
from typing import Dict, List
from datetime import datetime, timedelta

@dataclass
class PerformanceMetric:
    timestamp: datetime
    metric_name: str
    value: float
    tags: Dict[str, str]

class MetricsCollector:
    def __init__(self):
        self.metrics = []
        self.collectors = {}
        self.collection_interval = 60  # 1ë¶„
        self.running = False
    
    async def start_collection(self):
        """ë©”íŠ¸ë¦­ ìˆ˜ì§‘ ì‹œì‘"""
        self.running = True
        
        # ê¸°ë³¸ ì‹œìŠ¤í…œ ë©”íŠ¸ë¦­ ìˆ˜ì§‘ê¸° ë“±ë¡
        self.register_collector('system', self.collect_system_metrics)
        self.register_collector('application', self.collect_application_metrics)
        self.register_collector('database', self.collect_database_metrics)
        
        # ìˆ˜ì§‘ ë£¨í”„ ì‹œì‘
        await self.collection_loop()
    
    def register_collector(self, name, collector_func):
        """ë©”íŠ¸ë¦­ ìˆ˜ì§‘ê¸° ë“±ë¡"""
        self.collectors[name] = collector_func
    
    async def collection_loop(self):
        """ë©”íŠ¸ë¦­ ìˆ˜ì§‘ ë£¨í”„"""
        while self.running:
            try:
                for name, collector in self.collectors.items():
                    metrics = await collector()
                    self.metrics.extend(metrics)
                
                # ì˜¤ë˜ëœ ë©”íŠ¸ë¦­ ì •ë¦¬ (24ì‹œê°„ ì´ìƒ)
                cutoff_time = datetime.now() - timedelta(hours=24)
                self.metrics = [
                    m for m in self.metrics 
                    if m.timestamp > cutoff_time
                ]
                
                await asyncio.sleep(self.collection_interval)
                
            except Exception as e:
                logger.error(f"Metrics collection error: {str(e)}")
                await asyncio.sleep(5)
    
    async def collect_system_metrics(self) -> List[PerformanceMetric]:
        """ì‹œìŠ¤í…œ ë©”íŠ¸ë¦­ ìˆ˜ì§‘"""
        timestamp = datetime.now()
        metrics = []
        
        # CPU ì‚¬ìš©ë¥ 
        cpu_percent = psutil.cpu_percent(interval=1)
        metrics.append(PerformanceMetric(
            timestamp=timestamp,
            metric_name='cpu_usage_percent',
            value=cpu_percent,
            tags={'type': 'system'}
        ))
        
        # ë©”ëª¨ë¦¬ ì‚¬ìš©ë¥ 
        memory = psutil.virtual_memory()
        metrics.append(PerformanceMetric(
            timestamp=timestamp,
            metric_name='memory_usage_percent',
            value=memory.percent,
            tags={'type': 'system'}
        ))
        
        # ë””ìŠ¤í¬ I/O
        disk_io = psutil.disk_io_counters()
        if disk_io:
            metrics.append(PerformanceMetric(
                timestamp=timestamp,
                metric_name='disk_read_bytes',
                value=disk_io.read_bytes,
                tags={'type': 'system', 'io': 'read'}
            ))
            
            metrics.append(PerformanceMetric(
                timestamp=timestamp,
                metric_name='disk_write_bytes',
                value=disk_io.write_bytes,
                tags={'type': 'system', 'io': 'write'}
            ))
        
        # ë„¤íŠ¸ì›Œí¬ I/O
        network_io = psutil.net_io_counters()
        if network_io:
            metrics.append(PerformanceMetric(
                timestamp=timestamp,
                metric_name='network_bytes_sent',
                value=network_io.bytes_sent,
                tags={'type': 'system', 'direction': 'out'}
            ))
            
            metrics.append(PerformanceMetric(
                timestamp=timestamp,
                metric_name='network_bytes_recv',
                value=network_io.bytes_recv,
                tags={'type': 'system', 'direction': 'in'}
            ))
        
        return metrics
    
    async def collect_application_metrics(self) -> List[PerformanceMetric]:
        """ì• í”Œë¦¬ì¼€ì´ì…˜ ë©”íŠ¸ë¦­ ìˆ˜ì§‘"""
        timestamp = datetime.now()
        metrics = []
        
        # í™œì„± ì—°ê²° ìˆ˜
        try:
            from src.database.connection_pool import connection_pool
            if connection_pool:
                pool_stats = await connection_pool.monitor_pool_health()
                
                metrics.append(PerformanceMetric(
                    timestamp=timestamp,
                    metric_name='db_pool_size',
                    value=pool_stats['size'],
                    tags={'type': 'database', 'component': 'pool'}
                ))
                
                metrics.append(PerformanceMetric(
                    timestamp=timestamp,
                    metric_name='db_pool_usage_percent',
                    value=pool_stats['usage_percentage'],
                    tags={'type': 'database', 'component': 'pool'}
                ))
        except Exception:
            pass
        
        # í í¬ê¸°
        try:
            from src.queue.queue_manager import queue_manager
            queue_sizes = await queue_manager.get_queue_sizes()
            
            for queue_name, size in queue_sizes.items():
                metrics.append(PerformanceMetric(
                    timestamp=timestamp,
                    metric_name='queue_size',
                    value=size,
                    tags={'type': 'queue', 'queue_name': queue_name}
                ))
        except Exception:
            pass
        
        return metrics
    
    async def collect_database_metrics(self) -> List[PerformanceMetric]:
        """ë°ì´í„°ë² ì´ìŠ¤ ë©”íŠ¸ë¦­ ìˆ˜ì§‘"""
        timestamp = datetime.now()
        metrics = []
        
        try:
            from src.database.db_manager import db_manager
            
            # í™œì„± ì—°ê²° ìˆ˜
            active_connections = await db_manager.get_active_connections()
            metrics.append(PerformanceMetric(
                timestamp=timestamp,
                metric_name='db_active_connections',
                value=active_connections,
                tags={'type': 'database'}
            ))
            
            # ëŠë¦° ì¿¼ë¦¬ ìˆ˜
            slow_queries = await db_manager.get_slow_query_count()
            metrics.append(PerformanceMetric(
                timestamp=timestamp,
                metric_name='db_slow_queries',
                value=slow_queries,
                tags={'type': 'database'}
            ))
            
        except Exception as e:
            logger.warning(f"Database metrics collection failed: {str(e)}")
        
        return metrics
    
    def get_metrics_summary(self, time_window=3600) -> Dict:
        """ë©”íŠ¸ë¦­ ìš”ì•½ í†µê³„"""
        cutoff_time = datetime.now() - timedelta(seconds=time_window)
        recent_metrics = [
            m for m in self.metrics 
            if m.timestamp > cutoff_time
        ]
        
        summary = {}
        
        # ë©”íŠ¸ë¦­ë³„ í†µê³„
        for metric in recent_metrics:
            name = metric.metric_name
            if name not in summary:
                summary[name] = {
                    'values': [],
                    'count': 0,
                    'tags': metric.tags
                }
            
            summary[name]['values'].append(metric.value)
            summary[name]['count'] += 1
        
        # í†µê³„ ê³„ì‚°
        for name, data in summary.items():
            values = data['values']
            if values:
                data.update({
                    'avg': sum(values) / len(values),
                    'min': min(values),
                    'max': max(values),
                    'latest': values[-1]
                })
        
        return summary
```

### ì„±ëŠ¥ ì•Œë¦¼ ì‹œìŠ¤í…œ
```python
# src/optimization/performance_alerting.py
class PerformanceAlerting:
    def __init__(self, metrics_collector, notification_manager):
        self.metrics = metrics_collector
        self.notifications = notification_manager
        self.alert_rules = {}
        self.alert_history = {}
        self.cooldown_period = 300  # 5ë¶„
    
    def add_alert_rule(self, name, metric_name, condition, threshold, severity='warning'):
        """ì•Œë¦¼ ê·œì¹™ ì¶”ê°€"""
        self.alert_rules[name] = {
            'metric_name': metric_name,
            'condition': condition,  # 'gt', 'lt', 'eq'
            'threshold': threshold,
            'severity': severity,
            'enabled': True
        }
    
    async def check_alerts(self):
        """ì•Œë¦¼ ì¡°ê±´ í™•ì¸"""
        current_time = datetime.now()
        metrics_summary = self.metrics.get_metrics_summary(300)  # 5ë¶„ ìœˆë„ìš°
        
        for rule_name, rule in self.alert_rules.items():
            if not rule['enabled']:
                continue
            
            metric_data = metrics_summary.get(rule['metric_name'])
            if not metric_data:
                continue
            
            current_value = metric_data['latest']
            threshold = rule['threshold']
            condition = rule['condition']
            
            # ì¡°ê±´ í™•ì¸
            triggered = False
            if condition == 'gt' and current_value > threshold:
                triggered = True
            elif condition == 'lt' and current_value < threshold:
                triggered = True
            elif condition == 'eq' and current_value == threshold:
                triggered = True
            
            if triggered:
                await self.handle_alert(rule_name, rule, current_value, current_time)
    
    async def handle_alert(self, rule_name, rule, current_value, timestamp):
        """ì•Œë¦¼ ì²˜ë¦¬"""
        # ì¿¨ë‹¤ìš´ ì²´í¬
        if rule_name in self.alert_history:
            last_alert = self.alert_history[rule_name]
            if (timestamp - last_alert).seconds < self.cooldown_period:
                return
        
        # ì•Œë¦¼ ë°œì†¡
        message = f"""
ğŸš¨ ì„±ëŠ¥ ì•Œë¦¼: {rule_name}

ë©”íŠ¸ë¦­: {rule['metric_name']}
í˜„ì¬ê°’: {current_value}
ì„ê³„ê°’: {rule['threshold']}
ì‹¬ê°ë„: {rule['severity']}
ì‹œê°„: {timestamp.strftime('%Y-%m-%d %H:%M:%S')}
        """
        
        await self.notifications.send_alert(rule['severity'], message)
        
        # ì•Œë¦¼ ê¸°ë¡
        self.alert_history[rule_name] = timestamp
    
    def setup_default_rules(self):
        """ê¸°ë³¸ ì•Œë¦¼ ê·œì¹™ ì„¤ì •"""
        # CPU ì‚¬ìš©ë¥  ì•Œë¦¼
        self.add_alert_rule(
            'high_cpu_usage',
            'cpu_usage_percent',
            'gt',
            80,
            'warning'
        )
        
        # ë©”ëª¨ë¦¬ ì‚¬ìš©ë¥  ì•Œë¦¼
        self.add_alert_rule(
            'high_memory_usage',
            'memory_usage_percent',
            'gt',
            85,
            'critical'
        )
        
        # ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ìˆ˜ ì•Œë¦¼
        self.add_alert_rule(
            'high_db_connections',
            'db_active_connections',
            'gt',
            50,
            'warning'
        )
        
        # í ì ì²´ ì•Œë¦¼
        self.add_alert_rule(
            'queue_backlog',
            'queue_size',
            'gt',
            1000,
            'critical'
        )
```

ì´ ì„±ëŠ¥ ìµœì í™” ê°€ì´ë“œë¥¼ í†µí•´ ë“œëì‰¬í•‘ ìë™í™” ì‹œìŠ¤í…œì˜ ëª¨ë“  ì¸¡ë©´ì—ì„œ ìµœì ì˜ ì„±ëŠ¥ì„ ë‹¬ì„±í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì§€ì†ì ì¸ ëª¨ë‹ˆí„°ë§ê³¼ ì ì§„ì  ê°œì„ ì„ í†µí•´ ì‹œìŠ¤í…œì˜ íš¨ìœ¨ì„±ì„ ê·¹ëŒ€í™”í•˜ì‹œê¸° ë°”ëë‹ˆë‹¤.