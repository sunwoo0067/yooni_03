#!/usr/bin/env python3
"""
ë“œë¡­ì‰¬í•‘ ì‹œìŠ¤í…œ ì„±ëŠ¥ ìµœì í™” ì ìš© ìŠ¤í¬ë¦½íŠ¸
ë°ì´í„°ë² ì´ìŠ¤ ì¸ë±ìŠ¤, ìºì‹œ ì„¤ì •, API ìµœì í™”ë¥¼ ìë™ìœ¼ë¡œ ì ìš©
"""

import asyncio
import sys
import logging
from datetime import datetime
from typing import Dict, Any, List

# í”„ë¡œì íŠ¸ ëª¨ë“ˆ import
sys.path.append('.')

from app.core.config import settings
from app.services.database.database import engine, async_engine, db_manager
from app.models.performance_indexes import create_performance_indexes
from app.services.performance.enhanced_cache_manager import enhanced_cache_manager
from app.services.performance.database_optimizer import db_optimizer
from app.services.performance.async_batch_processor import wholesaler_batch_processor

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.StreamHandler(),
        logging.FileHandler('performance_optimization.log')
    ]
)
logger = logging.getLogger(__name__)


class PerformanceOptimizationMigrator:
    """ì„±ëŠ¥ ìµœì í™” ë§ˆì´ê·¸ë ˆì´ì…˜"""
    
    def __init__(self):
        self.optimization_steps = [
            ("ë°ì´í„°ë² ì´ìŠ¤ ì¸ë±ìŠ¤ ìƒì„±", self.create_database_indexes),
            ("ìºì‹œ ì‹œìŠ¤í…œ ì´ˆê¸°í™”", self.initialize_cache_system),
            ("ì—°ê²° í’€ ìµœì í™”", self.optimize_connection_pools),
            ("ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ ì„¤ì •", self.setup_performance_monitoring),
            ("ê¸°ë³¸ ìºì‹œ ë°ì´í„° ë¡œë”©", self.warm_up_cache),
            ("ìµœì í™” ê²€ì¦", self.verify_optimizations)
        ]
        
        self.results = {
            "started_at": datetime.now().isoformat(),
            "steps": {},
            "overall_success": False,
            "errors": [],
            "warnings": []
        }
    
    async def run_optimization(self) -> Dict[str, Any]:
        """ì „ì²´ ìµœì í™” í”„ë¡œì„¸ìŠ¤ ì‹¤í–‰"""
        logger.info("=== ë“œë¡­ì‰¬í•‘ ì‹œìŠ¤í…œ ì„±ëŠ¥ ìµœì í™” ì‹œì‘ ===")
        
        try:
            for step_name, step_func in self.optimization_steps:
                logger.info(f"ë‹¨ê³„ ì‹¤í–‰: {step_name}")
                
                try:
                    step_result = await step_func()
                    self.results["steps"][step_name] = {
                        "success": True,
                        "result": step_result,
                        "completed_at": datetime.now().isoformat()
                    }
                    logger.info(f"âœ… {step_name} ì™„ë£Œ")
                    
                except Exception as e:
                    error_msg = f"{step_name} ì‹¤íŒ¨: {str(e)}"
                    logger.error(error_msg)
                    self.results["steps"][step_name] = {
                        "success": False,
                        "error": str(e),
                        "completed_at": datetime.now().isoformat()
                    }
                    self.results["errors"].append(error_msg)
            
            # ì „ì²´ ì„±ê³µ ì—¬ë¶€ ê²°ì •
            successful_steps = sum(1 for step in self.results["steps"].values() if step["success"])
            total_steps = len(self.optimization_steps)
            
            self.results["overall_success"] = successful_steps == total_steps
            self.results["success_rate"] = successful_steps / total_steps * 100
            
            logger.info(f"=== ìµœì í™” ì™„ë£Œ: {successful_steps}/{total_steps} ë‹¨ê³„ ì„±ê³µ ===")
            
        except Exception as e:
            logger.error(f"ìµœì í™” í”„ë¡œì„¸ìŠ¤ ì¤‘ ì¹˜ëª…ì  ì˜¤ë¥˜: {e}")
            self.results["fatal_error"] = str(e)
        
        finally:
            self.results["completed_at"] = datetime.now().isoformat()
            await self.cleanup()
        
        return self.results
    
    async def create_database_indexes(self) -> Dict[str, Any]:
        """ë°ì´í„°ë² ì´ìŠ¤ ì„±ëŠ¥ ì¸ë±ìŠ¤ ìƒì„±"""
        logger.info("ë°ì´í„°ë² ì´ìŠ¤ ì¸ë±ìŠ¤ ìƒì„± ì¤‘...")
        
        try:
            # ë™ê¸° ì—”ì§„ìœ¼ë¡œ ì¸ë±ìŠ¤ ìƒì„±
            result = create_performance_indexes(engine)
            
            # ì¸ë±ìŠ¤ ì‚¬ìš©ëŸ‰ í†µê³„ í™œì„±í™” (PostgreSQL)
            if 'postgresql' in str(engine.url):
                with engine.connect() as conn:
                    try:
                        # pg_stat_statements í™•ì¥ í™œì„±í™”
                        conn.execute("CREATE EXTENSION IF NOT EXISTS pg_stat_statements;")
                        logger.info("pg_stat_statements í™•ì¥ í™œì„±í™”ë¨")
                    except Exception as e:
                        logger.warning(f"pg_stat_statements í™œì„±í™” ì‹¤íŒ¨: {e}")
            
            return {
                "created_indexes": result["created_count"],
                "total_indexes": result["total_indexes"],
                "errors": result["errors"],
                "database_type": str(engine.url).split(':')[0]
            }
            
        except Exception as e:
            logger.error(f"ì¸ë±ìŠ¤ ìƒì„± ì‹¤íŒ¨: {e}")
            raise
    
    async def initialize_cache_system(self) -> Dict[str, Any]:
        """ìºì‹œ ì‹œìŠ¤í…œ ì´ˆê¸°í™”"""
        logger.info("ìºì‹œ ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì¤‘...")
        
        try:
            # Redis ì—°ê²° í…ŒìŠ¤íŠ¸
            test_key = "optimization_test"
            test_value = {"test": True, "timestamp": datetime.now().isoformat()}
            
            success = enhanced_cache_manager.set(test_key, test_value)
            if not success:
                raise Exception("Redis ì—°ê²° ì‹¤íŒ¨")
            
            retrieved = enhanced_cache_manager.get(test_key)
            if not retrieved:
                raise Exception("Redis ì½ê¸° ì‹¤íŒ¨")
            
            # í…ŒìŠ¤íŠ¸ í‚¤ ì‚­ì œ
            enhanced_cache_manager.delete(test_key)
            
            # ê¸°ë³¸ ì˜ì¡´ì„± ê·œì¹™ í™•ì¸
            from app.services.performance.enhanced_cache_manager import setup_default_dependency_rules
            setup_default_dependency_rules()
            
            # ë„¤ì„ìŠ¤í˜ì´ìŠ¤ë³„ TTL ìµœì í™”
            namespace_configs = {
                "products": {"ttl": 600},      # 10ë¶„
                "orders": {"ttl": 300},        # 5ë¶„
                "analytics": {"ttl": 1800},    # 30ë¶„
                "users": {"ttl": 3600},        # 1ì‹œê°„
                "inventory": {"ttl": 180},     # 3ë¶„
                "search": {"ttl": 900}         # 15ë¶„
            }
            
            return {
                "redis_connection": "success",
                "compression_enabled": enhanced_cache_manager.compression_enabled,
                "cluster_mode": enhanced_cache_manager.is_cluster if hasattr(enhanced_cache_manager, 'is_cluster') else False,
                "namespace_configs": namespace_configs,
                "dependency_rules": len(enhanced_cache_manager._dependency_graph)
            }
            
        except Exception as e:
            logger.error(f"ìºì‹œ ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            raise
    
    async def optimize_connection_pools(self) -> Dict[str, Any]:
        """ì—°ê²° í’€ ìµœì í™”"""
        logger.info("ì—°ê²° í’€ ìµœì í™” ì¤‘...")
        
        try:
            # ë„ë§¤ì²˜ API ì—°ê²° í’€ ì„¤ì •
            wholesaler_batch_processor._setup_connection_pools()
            
            # ê° í’€ í…ŒìŠ¤íŠ¸
            pool_results = {}
            for pool_name in ["ownerclan", "zentrade", "domeggook"]:
                try:
                    session = await wholesaler_batch_processor.connection_manager.get_session(pool_name)
                    pool_results[pool_name] = {
                        "status": "configured",
                        "connector_limit": session.connector.limit,
                        "connector_limit_per_host": session.connector.limit_per_host
                    }
                except Exception as e:
                    pool_results[pool_name] = {
                        "status": "error",
                        "error": str(e)
                    }
            
            # ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° í’€ í™•ì¸
            db_pool_info = {
                "sync_pool_size": engine.pool.size(),
                "sync_pool_checked_in": engine.pool.checkedin(),
                "sync_pool_checked_out": engine.pool.checkedout(),
            }
            
            if async_engine:
                db_pool_info.update({
                    "async_pool_available": True,
                    "async_pool_size": async_engine.pool.size(),
                })
            
            return {
                "wholesaler_pools": pool_results,
                "database_pool": db_pool_info,
                "optimization_applied": True
            }
            
        except Exception as e:
            logger.error(f"ì—°ê²° í’€ ìµœì í™” ì‹¤íŒ¨: {e}")
            raise
    
    async def setup_performance_monitoring(self) -> Dict[str, Any]:
        """ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ ì„¤ì •"""
        logger.info("ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ ì„¤ì • ì¤‘...")
        
        try:
            # ëª¨ë‹ˆí„°ë§ ì‹œìŠ¤í…œ ì´ˆê¸°í™”
            from app.services.performance.async_batch_processor import performance_monitor
            
            # ì´ˆê¸° ì„±ëŠ¥ ê¸°ì¤€ì„  ì„¤ì •
            baseline_metrics = {
                "api_response_time_target": 0.2,  # 200ms
                "database_query_time_target": 0.05,  # 50ms
                "cache_hit_rate_target": 0.8,  # 80%
                "error_rate_threshold": 0.05  # 5%
            }
            
            # ì„±ëŠ¥ ì§€í‘œ ìˆ˜ì§‘ ì‹œì‘
            performance_monitor.clear_metrics()
            
            # ë°ì´í„°ë² ì´ìŠ¤ ì˜µí‹°ë§ˆì´ì € ì´ˆê¸°í™”
            db_optimizer.clear_stats()
            
            return {
                "monitoring_enabled": True,
                "baseline_metrics": baseline_metrics,
                "performance_targets": {
                    "api_response_time": "< 200ms",
                    "database_query_time": "< 50ms",
                    "cache_hit_rate": "> 80%",
                    "concurrent_users": 100
                }
            }
            
        except Exception as e:
            logger.error(f"ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ ì„¤ì • ì‹¤íŒ¨: {e}")
            raise
    
    async def warm_up_cache(self) -> Dict[str, Any]:
        """ìºì‹œ ì›Œë°ì—…"""
        logger.info("ìºì‹œ ì›Œë°ì—… ì¤‘...")
        
        try:
            # ì£¼ìš” ìºì‹œ ë°ì´í„° ì‚¬ì „ ë¡œë”©
            cache_configs = {
                "categories": {
                    "data_loader": self._load_product_categories,
                    "ttl": 3600
                },
                "user_settings": {
                    "data_loader": self._load_user_settings,
                    "ttl": 1800
                },
                "system_config": {
                    "data_loader": self._load_system_config,
                    "ttl": 7200
                }
            }
            
            warmed_counts = enhanced_cache_manager.warm_up_cache(cache_configs)
            
            return {
                "warmed_namespaces": list(warmed_counts.keys()),
                "warmed_counts": warmed_counts,
                "total_warmed": sum(warmed_counts.values())
            }
            
        except Exception as e:
            logger.error(f"ìºì‹œ ì›Œë°ì—… ì‹¤íŒ¨: {e}")
            raise
    
    async def verify_optimizations(self) -> Dict[str, Any]:
        """ìµœì í™” ê²€ì¦"""
        logger.info("ìµœì í™” ê²°ê³¼ ê²€ì¦ ì¤‘...")
        
        try:
            verification_results = {}
            
            # 1. ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° í…ŒìŠ¤íŠ¸
            db_test = db_manager.check_connection()
            verification_results["database_connection"] = db_test
            
            if async_engine:
                async_db_test = await db_manager.check_connection_async()
                verification_results["async_database_connection"] = async_db_test
            
            # 2. ìºì‹œ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸
            cache_stats = enhanced_cache_manager.get_performance_stats()
            verification_results["cache_system"] = {
                "operational": cache_stats.get("redis_memory_usage") != "Unknown",
                "l1_cache_size": cache_stats.get("l1_cache_size", 0),
                "compression_enabled": cache_stats.get("compression_efficiency") != "N/A"
            }
            
            # 3. API ì—°ê²° í’€ í…ŒìŠ¤íŠ¸
            pool_tests = {}
            for pool_name in ["ownerclan", "zentrade", "domeggook"]:
                try:
                    session = await wholesaler_batch_processor.connection_manager.get_session(pool_name)
                    pool_tests[pool_name] = True
                except:
                    pool_tests[pool_name] = False
            
            verification_results["api_connection_pools"] = pool_tests
            
            # 4. ì„±ëŠ¥ ê¸°ì¤€ í™•ì¸
            performance_check = {
                "database_optimizer": hasattr(db_optimizer, 'query_stats'),
                "cache_manager": hasattr(enhanced_cache_manager, '_memory_cache'),
                "performance_monitor": hasattr(performance_monitor, 'metrics')
            }
            verification_results["performance_systems"] = performance_check
            
            # ì „ì²´ ì„±ê³µ ì—¬ë¶€
            all_systems_ok = (
                verification_results["database_connection"] and
                verification_results["cache_system"]["operational"] and
                all(pool_tests.values()) and
                all(performance_check.values())
            )
            
            verification_results["overall_status"] = "success" if all_systems_ok else "partial"
            
            return verification_results
            
        except Exception as e:
            logger.error(f"ìµœì í™” ê²€ì¦ ì‹¤íŒ¨: {e}")
            raise
    
    def _load_product_categories(self) -> Dict[str, Any]:
        """ìƒí’ˆ ì¹´í…Œê³ ë¦¬ ë°ì´í„° ë¡œë”"""
        return {
            "electronics": {"name": "ì „ìì œí’ˆ", "priority": 1},
            "fashion": {"name": "íŒ¨ì…˜", "priority": 2},
            "home": {"name": "í™ˆ&ë¦¬ë¹™", "priority": 3},
            "beauty": {"name": "ë·°í‹°", "priority": 4},
            "sports": {"name": "ìŠ¤í¬ì¸ ", "priority": 5}
        }
    
    def _load_user_settings(self) -> Dict[str, Any]:
        """ì‚¬ìš©ì ì„¤ì • ë°ì´í„° ë¡œë”"""
        return {
            "default_margin": 30,
            "auto_price_update": True,
            "notification_enabled": True,
            "sync_interval": 3600
        }
    
    def _load_system_config(self) -> Dict[str, Any]:
        """ì‹œìŠ¤í…œ ì„¤ì • ë°ì´í„° ë¡œë”"""
        return {
            "maintenance_mode": False,
            "api_rate_limits": {
                "ownerclan": 60,
                "zentrade": 30,
                "domeggook": 100
            },
            "cache_ttl_defaults": {
                "products": 600,
                "orders": 300,
                "analytics": 1800
            }
        }
    
    async def cleanup(self):
        """ë¦¬ì†ŒìŠ¤ ì •ë¦¬"""
        try:
            await wholesaler_batch_processor.cleanup()
            logger.info("ë¦¬ì†ŒìŠ¤ ì •ë¦¬ ì™„ë£Œ")
        except Exception as e:
            logger.warning(f"ë¦¬ì†ŒìŠ¤ ì •ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
    
    def generate_report(self) -> str:
        """ìµœì í™” ë³´ê³ ì„œ ìƒì„±"""
        report = []
        report.append("=" * 60)
        report.append("ë“œë¡­ì‰¬í•‘ ì‹œìŠ¤í…œ ì„±ëŠ¥ ìµœì í™” ë³´ê³ ì„œ")
        report.append("=" * 60)
        report.append(f"ì‹œì‘ ì‹œê°„: {self.results['started_at']}")
        report.append(f"ì™„ë£Œ ì‹œê°„: {self.results.get('completed_at', 'N/A')}")
        report.append(f"ì „ì²´ ì„±ê³µ: {'âœ…' if self.results['overall_success'] else 'âŒ'}")
        report.append(f"ì„±ê³µë¥ : {self.results.get('success_rate', 0):.1f}%")
        report.append("")
        
        # ë‹¨ê³„ë³„ ê²°ê³¼
        report.append("ë‹¨ê³„ë³„ ì‹¤í–‰ ê²°ê³¼:")
        report.append("-" * 40)
        for step_name, step_result in self.results["steps"].items():
            status = "âœ…" if step_result["success"] else "âŒ"
            report.append(f"{status} {step_name}")
            if not step_result["success"]:
                report.append(f"   ì˜¤ë¥˜: {step_result.get('error', 'Unknown')}")
        
        report.append("")
        
        # ì„±ëŠ¥ ê°œì„  ì‚¬í•­
        report.append("ì ìš©ëœ ì„±ëŠ¥ ìµœì í™”:")
        report.append("-" * 40)
        report.append("â€¢ ë°ì´í„°ë² ì´ìŠ¤ ì¸ë±ìŠ¤ ìµœì í™” (N+1 ì¿¼ë¦¬ í•´ê²°)")
        report.append("â€¢ Redis ê¸°ë°˜ 2ë‹¨ê³„ ìºì‹± (L1 ë©”ëª¨ë¦¬ + L2 Redis)")
        report.append("â€¢ ì••ì¶• ê¸°ë°˜ ìºì‹œ ì €ì¥ (ë©”ëª¨ë¦¬ ì ˆì•½)")
        report.append("â€¢ ë¹„ë™ê¸° ë°°ì¹˜ ì²˜ë¦¬ (ì™¸ë¶€ API ìµœì í™”)")
        report.append("â€¢ ì—°ê²° í’€ë§ (ë°ì´í„°ë² ì´ìŠ¤ + API)")
        report.append("â€¢ ì‹¤ì‹œê°„ ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§")
        report.append("")
        
        # ê¸°ëŒ€ íš¨ê³¼
        report.append("ê¸°ëŒ€ ì„±ëŠ¥ ê°œì„  íš¨ê³¼:")
        report.append("-" * 40)
        report.append("â€¢ API ì‘ë‹µ ì‹œê°„: 50-70% í–¥ìƒ (< 200ms ëª©í‘œ)")
        report.append("â€¢ ë°ì´í„°ë² ì´ìŠ¤ ì¿¼ë¦¬: 80-90% í–¥ìƒ (< 50ms ëª©í‘œ)")
        report.append("â€¢ ìºì‹œ íˆíŠ¸ìœ¨: > 80% ë‹¬ì„±")
        report.append("â€¢ ë™ì‹œ ì‚¬ìš©ì: 100ëª… ì§€ì›")
        report.append("â€¢ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: 30-50% ì ˆì•½")
        report.append("")
        
        # ì—ëŸ¬ ë° ê²½ê³ 
        if self.results["errors"]:
            report.append("ë°œìƒí•œ ì˜¤ë¥˜:")
            report.append("-" * 40)
            for error in self.results["errors"]:
                report.append(f"â€¢ {error}")
            report.append("")
        
        if self.results["warnings"]:
            report.append("ê²½ê³  ì‚¬í•­:")
            report.append("-" * 40)
            for warning in self.results["warnings"]:
                report.append(f"â€¢ {warning}")
            report.append("")
        
        # ë‹¤ìŒ ë‹¨ê³„ ê¶Œì¥ì‚¬í•­
        report.append("ë‹¤ìŒ ë‹¨ê³„ ê¶Œì¥ì‚¬í•­:")
        report.append("-" * 40)
        report.append("1. ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ ëŒ€ì‹œë³´ë“œ í™•ì¸")
        report.append("   â†’ GET /api/v1/performance/overview")
        report.append("2. ì‹¤ì œ ì›Œí¬ë¡œë“œë¡œ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ ì‹¤í–‰")
        report.append("   â†’ POST /api/v1/performance/benchmark")
        report.append("3. ì •ê¸°ì ì¸ ì„±ëŠ¥ ë¦¬í¬íŠ¸ ê²€í† ")
        report.append("4. í•„ìš”ì‹œ ì¶”ê°€ ì¸ë±ìŠ¤ ë˜ëŠ” ìºì‹œ ì •ì±… ì¡°ì •")
        report.append("")
        
        report.append("=" * 60)
        
        return "\n".join(report)


async def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    print("ğŸš€ ë“œë¡­ì‰¬í•‘ ì‹œìŠ¤í…œ ì„±ëŠ¥ ìµœì í™”ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤...")
    print()
    
    migrator = PerformanceOptimizationMigrator()
    
    try:
        # ìµœì í™” ì‹¤í–‰
        results = await migrator.run_optimization()
        
        # ë³´ê³ ì„œ ìƒì„± ë° ì¶œë ¥
        report = migrator.generate_report()
        print(report)
        
        # íŒŒì¼ë¡œ ì €ì¥
        report_filename = f"performance_optimization_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.txt"
        with open(report_filename, 'w', encoding='utf-8') as f:
            f.write(report)
        
        print(f"\nğŸ“„ ìƒì„¸ ë³´ê³ ì„œê°€ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤: {report_filename}")
        
        # ì„±ê³µ/ì‹¤íŒ¨ì— ë”°ë¥¸ exit code
        if results["overall_success"]:
            print("\nğŸ‰ ì„±ëŠ¥ ìµœì í™”ê°€ ì„±ê³µì ìœ¼ë¡œ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")
            print("\në‹¤ìŒ ëª…ë ¹ì–´ë¡œ ì„±ëŠ¥ ìƒíƒœë¥¼ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:")
            print("curl http://localhost:8000/api/v1/performance/overview")
            return 0
        else:
            print("\nâš ï¸ ì¼ë¶€ ìµœì í™” ë‹¨ê³„ì—ì„œ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.")
            print("ë³´ê³ ì„œë¥¼ í™•ì¸í•˜ê³  í•„ìš”í•œ ì¡°ì¹˜ë¥¼ ì·¨í•˜ì„¸ìš”.")
            return 1
            
    except KeyboardInterrupt:
        print("\n\nâŒ ì‚¬ìš©ìì— ì˜í•´ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤.")
        return 130
    except Exception as e:
        print(f"\n\nğŸ’¥ ì¹˜ëª…ì  ì˜¤ë¥˜ ë°œìƒ: {e}")
        logger.exception("Fatal error during optimization")
        return 1


if __name__ == "__main__":
    # ìµœì í™” ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰
    exit_code = asyncio.run(main())
    sys.exit(exit_code)