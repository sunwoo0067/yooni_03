# ğŸš€ í”„ë¡œë•ì…˜ ë°°í¬ ê°€ì´ë“œ

## ğŸ“‹ ëª©ì°¨

1. [ì‚¬ì „ ì¤€ë¹„](#ì‚¬ì „-ì¤€ë¹„)
2. [ì¸í”„ë¼ êµ¬ì„±](#ì¸í”„ë¼-êµ¬ì„±)
3. [ë³´ì•ˆ ì„¤ì •](#ë³´ì•ˆ-ì„¤ì •)
4. [ë°°í¬ í”„ë¡œì„¸ìŠ¤](#ë°°í¬-í”„ë¡œì„¸ìŠ¤)
5. [ëª¨ë‹ˆí„°ë§ ì„¤ì •](#ëª¨ë‹ˆí„°ë§-ì„¤ì •)
6. [ë°±ì—… ë° ë³µêµ¬](#ë°±ì—…-ë°-ë³µêµ¬)
7. [íŠ¸ëŸ¬ë¸”ìŠˆíŒ…](#íŠ¸ëŸ¬ë¸”ìŠˆíŒ…)
8. [ì²´í¬ë¦¬ìŠ¤íŠ¸](#ì²´í¬ë¦¬ìŠ¤íŠ¸)

## ğŸ”§ ì‚¬ì „ ì¤€ë¹„

### 1. ì‹œìŠ¤í…œ ìš”êµ¬ì‚¬í•­

#### ìµœì†Œ ì‚¬ì–‘
- **CPU**: 4 vCPU
- **Memory**: 8GB RAM
- **Storage**: 100GB SSD
- **Network**: 1Gbps

#### ê¶Œì¥ ì‚¬ì–‘
- **CPU**: 8 vCPU
- **Memory**: 16GB RAM
- **Storage**: 500GB SSD (RAID 1)
- **Network**: 10Gbps

### 2. ì†Œí”„íŠ¸ì›¨ì–´ ìš”êµ¬ì‚¬í•­

```bash
# ìš´ì˜ì²´ì œ
Ubuntu 22.04 LTS ë˜ëŠ” Amazon Linux 2023

# Docker & Docker Compose
Docker Engine 24.0+
Docker Compose 2.20+

# ë°ì´í„°ë² ì´ìŠ¤
PostgreSQL 15+
Redis 7+

# ëª¨ë‹ˆí„°ë§
Prometheus 2.45+
Grafana 10+
```

### 3. ë„ë©”ì¸ ë° SSL ì¸ì¦ì„œ

```bash
# ë„ë©”ì¸ ì„¤ì •
api.yourdomain.com      # API ì„œë²„
app.yourdomain.com      # í”„ë¡ íŠ¸ì—”ë“œ
monitor.yourdomain.com  # ëª¨ë‹ˆí„°ë§

# SSL ì¸ì¦ì„œ (Let's Encrypt)
sudo certbot certonly --standalone -d api.yourdomain.com
```

## ğŸ—ï¸ ì¸í”„ë¼ êµ¬ì„±

### 1. AWS ì¸í”„ë¼ (ê¶Œì¥)

```yaml
# terraform/main.tf
provider "aws" {
  region = "ap-northeast-2"
}

# VPC êµ¬ì„±
resource "aws_vpc" "main" {
  cidr_block           = "10.0.0.0/16"
  enable_dns_hostnames = true
  enable_dns_support   = true

  tags = {
    Name = "dropshipping-vpc"
  }
}

# ì„œë¸Œë„· êµ¬ì„± (Multi-AZ)
resource "aws_subnet" "public_a" {
  vpc_id                  = aws_vpc.main.id
  cidr_block              = "10.0.1.0/24"
  availability_zone       = "ap-northeast-2a"
  map_public_ip_on_launch = true
}

resource "aws_subnet" "public_c" {
  vpc_id                  = aws_vpc.main.id
  cidr_block              = "10.0.2.0/24"
  availability_zone       = "ap-northeast-2c"
  map_public_ip_on_launch = true
}

# RDS (PostgreSQL)
resource "aws_db_instance" "postgres" {
  identifier             = "dropshipping-db"
  engine                 = "postgres"
  engine_version         = "15.4"
  instance_class         = "db.t3.medium"
  allocated_storage      = 100
  storage_encrypted      = true
  vpc_security_group_ids = [aws_security_group.rds.id]
  db_subnet_group_name   = aws_db_subnet_group.main.name
  
  backup_retention_period = 30
  backup_window          = "03:00-04:00"
  maintenance_window     = "sun:04:00-sun:05:00"
  
  enabled_cloudwatch_logs_exports = ["postgresql"]
}

# ElastiCache (Redis)
resource "aws_elasticache_cluster" "redis" {
  cluster_id           = "dropshipping-cache"
  engine               = "redis"
  node_type            = "cache.t3.medium"
  num_cache_nodes      = 1
  parameter_group_name = "default.redis7"
  port                 = 6379
  
  snapshot_retention_limit = 7
  snapshot_window         = "03:00-05:00"
}

# ECS í´ëŸ¬ìŠ¤í„°
resource "aws_ecs_cluster" "main" {
  name = "dropshipping-cluster"

  setting {
    name  = "containerInsights"
    value = "enabled"
  }
}

# Application Load Balancer
resource "aws_lb" "main" {
  name               = "dropshipping-alb"
  internal           = false
  load_balancer_type = "application"
  security_groups    = [aws_security_group.alb.id]
  subnets            = [aws_subnet.public_a.id, aws_subnet.public_c.id]

  enable_deletion_protection = true
  enable_http2              = true
}
```

### 2. Docker Compose í”„ë¡œë•ì…˜ êµ¬ì„±

```yaml
# docker-compose.prod.yml
version: '3.9'

services:
  nginx:
    image: nginx:alpine
    container_name: nginx
    ports:
      - "80:80"
      - "443:443"
    volumes:
      - ./nginx/nginx.prod.conf:/etc/nginx/nginx.conf:ro
      - ./nginx/ssl:/etc/nginx/ssl:ro
      - static_volume:/app/static
    depends_on:
      - backend
      - frontend
    restart: always
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"

  backend:
    build:
      context: ./backend
      dockerfile: Dockerfile
    container_name: backend
    env_file:
      - .env.production
    volumes:
      - static_volume:/app/static
      - media_volume:/app/media
    depends_on:
      - postgres
      - redis
    restart: always
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 3
    deploy:
      replicas: 2
      resources:
        limits:
          cpus: '1'
          memory: 2G
        reservations:
          cpus: '0.5'
          memory: 1G

  frontend:
    build:
      context: ./frontend
      dockerfile: Dockerfile
      args:
        - VITE_API_URL=https://api.yourdomain.com
    container_name: frontend
    restart: always
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:3000/health"]
      interval: 30s
      timeout: 10s
      retries: 3

  postgres:
    image: postgres:15-alpine
    container_name: postgres
    env_file:
      - .env.production
    volumes:
      - postgres_data:/var/lib/postgresql/data
      - ./scripts/postgres-init.sql:/docker-entrypoint-initdb.d/init.sql
    restart: always
    command: >
      postgres
      -c max_connections=200
      -c shared_buffers=256MB
      -c effective_cache_size=1GB
      -c maintenance_work_mem=64MB
      -c checkpoint_completion_target=0.9
      -c wal_buffers=16MB
      -c default_statistics_target=100
      -c random_page_cost=1.1
      -c effective_io_concurrency=200
      -c work_mem=4MB
      -c min_wal_size=1GB
      -c max_wal_size=4GB

  redis:
    image: redis:7-alpine
    container_name: redis
    command: >
      redis-server
      --requirepass ${REDIS_PASSWORD}
      --maxmemory 512mb
      --maxmemory-policy allkeys-lru
      --save 900 1
      --save 300 10
      --save 60 10000
    volumes:
      - redis_data:/data
    restart: always

  prometheus:
    image: prom/prometheus:latest
    container_name: prometheus
    volumes:
      - ./monitoring/prometheus.yml:/etc/prometheus/prometheus.yml:ro
      - prometheus_data:/prometheus
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--storage.tsdb.path=/prometheus'
      - '--web.console.libraries=/usr/share/prometheus/console_libraries'
      - '--web.console.templates=/usr/share/prometheus/consoles'
      - '--web.enable-lifecycle'
      - '--storage.tsdb.retention.time=30d'
    restart: always

  grafana:
    image: grafana/grafana:latest
    container_name: grafana
    volumes:
      - grafana_data:/var/lib/grafana
      - ./monitoring/grafana/provisioning:/etc/grafana/provisioning
    environment:
      - GF_SECURITY_ADMIN_PASSWORD=${GRAFANA_PASSWORD}
      - GF_USERS_ALLOW_SIGN_UP=false
      - GF_INSTALL_PLUGINS=grafana-clock-panel,grafana-simple-json-datasource
    restart: always

volumes:
  postgres_data:
  redis_data:
  static_volume:
  media_volume:
  prometheus_data:
  grafana_data:

networks:
  default:
    driver: bridge
    ipam:
      config:
        - subnet: 172.20.0.0/16
```

### 3. Nginx í”„ë¡œë•ì…˜ ì„¤ì •

```nginx
# nginx/nginx.prod.conf
user nginx;
worker_processes auto;
error_log /var/log/nginx/error.log warn;
pid /var/run/nginx.pid;

events {
    worker_connections 4096;
    use epoll;
    multi_accept on;
}

http {
    include /etc/nginx/mime.types;
    default_type application/octet-stream;

    # ë¡œê¹… ì„¤ì •
    log_format main '$remote_addr - $remote_user [$time_local] "$request" '
                    '$status $body_bytes_sent "$http_referer" '
                    '"$http_user_agent" "$http_x_forwarded_for" '
                    'rt=$request_time uct="$upstream_connect_time" '
                    'uht="$upstream_header_time" urt="$upstream_response_time"';

    access_log /var/log/nginx/access.log main;

    # ì„±ëŠ¥ ìµœì í™”
    sendfile on;
    tcp_nopush on;
    tcp_nodelay on;
    keepalive_timeout 65;
    types_hash_max_size 2048;
    client_max_body_size 50M;

    # Gzip ì••ì¶•
    gzip on;
    gzip_vary on;
    gzip_min_length 1024;
    gzip_types text/plain text/css text/xml text/javascript 
               application/json application/javascript application/xml+rss;

    # ë³´ì•ˆ í—¤ë”
    add_header X-Frame-Options "SAMEORIGIN" always;
    add_header X-Content-Type-Options "nosniff" always;
    add_header X-XSS-Protection "1; mode=block" always;
    add_header Referrer-Policy "strict-origin-when-cross-origin" always;
    add_header Content-Security-Policy "default-src 'self' https:; script-src 'self' 'unsafe-inline' 'unsafe-eval' https:; style-src 'self' 'unsafe-inline' https:;" always;

    # Rate limiting
    limit_req_zone $binary_remote_addr zone=api:10m rate=10r/s;
    limit_req_zone $binary_remote_addr zone=login:10m rate=5r/m;

    # SSL ì„¤ì •
    ssl_protocols TLSv1.2 TLSv1.3;
    ssl_ciphers ECDHE-ECDSA-AES128-GCM-SHA256:ECDHE-RSA-AES128-GCM-SHA256:ECDHE-ECDSA-AES256-GCM-SHA384:ECDHE-RSA-AES256-GCM-SHA384;
    ssl_prefer_server_ciphers off;
    ssl_session_cache shared:SSL:10m;
    ssl_session_timeout 10m;
    ssl_stapling on;
    ssl_stapling_verify on;

    # Backend upstream
    upstream backend {
        least_conn;
        server backend:8000 max_fails=3 fail_timeout=30s;
        keepalive 32;
    }

    # Frontend upstream
    upstream frontend {
        server frontend:3000;
        keepalive 32;
    }

    # HTTP to HTTPS redirect
    server {
        listen 80;
        server_name api.yourdomain.com app.yourdomain.com;
        return 301 https://$server_name$request_uri;
    }

    # API Server
    server {
        listen 443 ssl http2;
        server_name api.yourdomain.com;

        ssl_certificate /etc/nginx/ssl/api.crt;
        ssl_certificate_key /etc/nginx/ssl/api.key;

        location / {
            proxy_pass http://backend;
            proxy_http_version 1.1;
            proxy_set_header Upgrade $http_upgrade;
            proxy_set_header Connection "upgrade";
            proxy_set_header Host $host;
            proxy_set_header X-Real-IP $remote_addr;
            proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
            proxy_set_header X-Forwarded-Proto $scheme;
            
            # Timeouts
            proxy_connect_timeout 60s;
            proxy_send_timeout 60s;
            proxy_read_timeout 60s;
            
            # Rate limiting
            limit_req zone=api burst=20 nodelay;
        }

        location /static/ {
            alias /app/static/;
            expires 30d;
            add_header Cache-Control "public, immutable";
        }

        location /media/ {
            alias /app/media/;
            expires 7d;
            add_header Cache-Control "public";
        }

        # Health check endpoint (no rate limiting)
        location /health {
            proxy_pass http://backend/health;
            access_log off;
        }
    }

    # Frontend Application
    server {
        listen 443 ssl http2;
        server_name app.yourdomain.com;

        ssl_certificate /etc/nginx/ssl/app.crt;
        ssl_certificate_key /etc/nginx/ssl/app.key;

        location / {
            proxy_pass http://frontend;
            proxy_http_version 1.1;
            proxy_set_header Host $host;
            proxy_set_header X-Real-IP $remote_addr;
            proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
            proxy_set_header X-Forwarded-Proto $scheme;
        }

        location ~* \.(js|css|png|jpg|jpeg|gif|ico|svg|woff|woff2|ttf|eot)$ {
            proxy_pass http://frontend;
            expires 1y;
            add_header Cache-Control "public, immutable";
        }
    }
}
```

## ğŸ”’ ë³´ì•ˆ ì„¤ì •

### 1. í™˜ê²½ ë³€ìˆ˜ ê´€ë¦¬

```bash
# .env.production (ì ˆëŒ€ Gitì— ì»¤ë°‹í•˜ì§€ ë§ ê²ƒ!)
# ë°ì´í„°ë² ì´ìŠ¤
DATABASE_URL=postgresql://dropship_user:strong_password@postgres:5432/dropship_prod
DATABASE_POOL_SIZE=20
DATABASE_MAX_OVERFLOW=40

# Redis
REDIS_URL=redis://:redis_password@redis:6379/0
REDIS_PASSWORD=strong_redis_password

# ë³´ì•ˆ
SECRET_KEY=your-very-long-random-secret-key-here
JWT_SECRET_KEY=another-very-long-random-jwt-secret
ENCRYPTION_KEY=base64-encoded-32-byte-key

# API Keys (í”„ë¡œë•ì…˜ìš©)
GEMINI_API_KEY=your-production-gemini-key
OPENAI_API_KEY=your-production-openai-key

# ë§ˆì¼“í”Œë ˆì´ìŠ¤ API
COUPANG_ACCESS_KEY=your-coupang-key
COUPANG_SECRET_KEY=your-coupang-secret
NAVER_CLIENT_ID=your-naver-id
NAVER_CLIENT_SECRET=your-naver-secret

# ëª¨ë‹ˆí„°ë§
SENTRY_DSN=https://xxx@sentry.io/xxx
DATADOG_API_KEY=your-datadog-key

# í™˜ê²½
ENVIRONMENT=production
DEBUG=false
LOG_LEVEL=INFO
```

### 2. ë°©í™”ë²½ ì„¤ì •

```bash
# UFW ì„¤ì •
sudo ufw default deny incoming
sudo ufw default allow outgoing

# SSH (IP ì œí•œ)
sudo ufw allow from YOUR_IP to any port 22

# HTTP/HTTPS
sudo ufw allow 80/tcp
sudo ufw allow 443/tcp

# ëª¨ë‹ˆí„°ë§ (ë‚´ë¶€ë§ë§Œ)
sudo ufw allow from 10.0.0.0/16 to any port 9090  # Prometheus
sudo ufw allow from 10.0.0.0/16 to any port 3000  # Grafana

# í™œì„±í™”
sudo ufw enable
```

### 3. SELinux/AppArmor ì„¤ì •

```bash
# SELinux (CentOS/RHEL)
sudo setsebool -P httpd_can_network_connect 1
sudo setsebool -P httpd_can_network_connect_db 1

# AppArmor (Ubuntu)
sudo aa-enforce /etc/apparmor.d/docker
```

## ğŸ“¦ ë°°í¬ í”„ë¡œì„¸ìŠ¤

### 1. ìë™ ë°°í¬ ìŠ¤í¬ë¦½íŠ¸

```bash
#!/bin/bash
# scripts/deploy.sh

set -e

echo "ğŸš€ Starting production deployment..."

# í™˜ê²½ ë³€ìˆ˜ í™•ì¸
if [ ! -f .env.production ]; then
    echo "âŒ .env.production file not found!"
    exit 1
fi

# ë°±ì—…
echo "ğŸ“¦ Creating backup..."
./scripts/backup-production.sh

# ì´ë¯¸ì§€ ë¹Œë“œ
echo "ğŸ”¨ Building Docker images..."
docker compose -f docker-compose.prod.yml build

# ë°ì´í„°ë² ì´ìŠ¤ ë§ˆì´ê·¸ë ˆì´ì…˜
echo "ğŸ—„ï¸ Running database migrations..."
docker compose -f docker-compose.prod.yml run --rm backend alembic upgrade head

# Blue-Green ë°°í¬
echo "ğŸ”„ Starting Blue-Green deployment..."

# ìƒˆ ì»¨í…Œì´ë„ˆ ì‹œì‘ (Green)
docker compose -f docker-compose.prod.yml up -d --scale backend=2 --no-deps backend_green

# í—¬ìŠ¤ì²´í¬
echo "ğŸ¥ Health checking new containers..."
./scripts/health-check.sh backend_green

# íŠ¸ë˜í”½ ì „í™˜
echo "ğŸ”€ Switching traffic..."
docker compose -f docker-compose.prod.yml exec nginx nginx -s reload

# ì´ì „ ì»¨í…Œì´ë„ˆ ì œê±°
echo "ğŸ§¹ Removing old containers..."
docker compose -f docker-compose.prod.yml stop backend_blue
docker compose -f docker-compose.prod.yml rm -f backend_blue

echo "âœ… Deployment completed successfully!"
```

### 2. ë¡¤ë°± ì ˆì°¨

```bash
#!/bin/bash
# scripts/rollback.sh

set -e

echo "ğŸ”™ Starting rollback..."

# ì´ì „ ë²„ì „ íƒœê·¸ í™•ì¸
PREVIOUS_VERSION=$(docker images --format "{{.Tag}}" | grep -E '^v[0-9]+\.[0-9]+\.[0-9]+$' | sort -V | tail -2 | head -1)

if [ -z "$PREVIOUS_VERSION" ]; then
    echo "âŒ No previous version found!"
    exit 1
fi

echo "ğŸ“Œ Rolling back to version: $PREVIOUS_VERSION"

# ì´ì „ ë²„ì „ìœ¼ë¡œ ì¬ë°°í¬
docker compose -f docker-compose.prod.yml up -d --no-deps \
    backend:$PREVIOUS_VERSION \
    frontend:$PREVIOUS_VERSION

# í—¬ìŠ¤ì²´í¬
./scripts/health-check.sh

echo "âœ… Rollback completed!"
```

### 3. ë°ì´í„°ë² ì´ìŠ¤ ë°±ì—… ë° ë³µêµ¬

```bash
#!/bin/bash
# scripts/backup-production.sh

set -e

BACKUP_DIR="/backups"
TIMESTAMP=$(date +%Y%m%d_%H%M%S)
BACKUP_FILE="$BACKUP_DIR/dropship_prod_$TIMESTAMP.sql"

echo "ğŸ—„ï¸ Creating database backup..."

# PostgreSQL ë°±ì—…
docker compose -f docker-compose.prod.yml exec -T postgres \
    pg_dump -U $POSTGRES_USER -d $POSTGRES_DB > $BACKUP_FILE

# ì••ì¶•
gzip $BACKUP_FILE

# S3 ì—…ë¡œë“œ (ì„ íƒì‚¬í•­)
aws s3 cp $BACKUP_FILE.gz s3://your-backup-bucket/database/

# ì˜¤ë˜ëœ ë°±ì—… ì‚­ì œ (30ì¼ ì´ìƒ)
find $BACKUP_DIR -name "*.sql.gz" -mtime +30 -delete

echo "âœ… Backup completed: $BACKUP_FILE.gz"
```

## ğŸ“Š ëª¨ë‹ˆí„°ë§ ì„¤ì •

### 1. Prometheus ì„¤ì •

```yaml
# monitoring/prometheus.yml
global:
  scrape_interval: 15s
  evaluation_interval: 15s

alerting:
  alertmanagers:
    - static_configs:
        - targets: ['alertmanager:9093']

rule_files:
  - "alerts/*.yml"

scrape_configs:
  - job_name: 'backend'
    static_configs:
      - targets: ['backend:8000']
    metrics_path: '/metrics'

  - job_name: 'nginx'
    static_configs:
      - targets: ['nginx-exporter:9113']

  - job_name: 'postgres'
    static_configs:
      - targets: ['postgres-exporter:9187']

  - job_name: 'redis'
    static_configs:
      - targets: ['redis-exporter:9121']

  - job_name: 'node'
    static_configs:
      - targets: ['node-exporter:9100']
```

### 2. ì•Œë¦¼ ê·œì¹™

```yaml
# monitoring/alerts/application.yml
groups:
  - name: application
    rules:
      - alert: HighErrorRate
        expr: rate(http_request_total{status=~"5.."}[5m]) > 0.05
        for: 5m
        labels:
          severity: critical
        annotations:
          summary: "High error rate detected"
          description: "Error rate is above 5% for 5 minutes"

      - alert: SlowResponseTime
        expr: histogram_quantile(0.95, http_request_duration_seconds_bucket) > 1
        for: 10m
        labels:
          severity: warning
        annotations:
          summary: "Slow response time"
          description: "95th percentile response time is above 1 second"

      - alert: HighMemoryUsage
        expr: (node_memory_MemTotal_bytes - node_memory_MemAvailable_bytes) / node_memory_MemTotal_bytes > 0.9
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "High memory usage"
          description: "Memory usage is above 90%"
```

### 3. Grafana ëŒ€ì‹œë³´ë“œ

```json
{
  "dashboard": {
    "title": "Dropshipping Production Dashboard",
    "panels": [
      {
        "title": "Request Rate",
        "targets": [
          {
            "expr": "rate(http_request_total[5m])",
            "legendFormat": "{{method}} {{endpoint}}"
          }
        ]
      },
      {
        "title": "Response Time",
        "targets": [
          {
            "expr": "histogram_quantile(0.95, http_request_duration_seconds_bucket)",
            "legendFormat": "95th percentile"
          }
        ]
      },
      {
        "title": "Error Rate",
        "targets": [
          {
            "expr": "rate(http_request_total{status=~\"5..\"}[5m])",
            "legendFormat": "5xx errors"
          }
        ]
      }
    ]
  }
}
```

## ğŸ’¾ ë°±ì—… ë° ë³µêµ¬

### 1. ìë™ ë°±ì—… ìŠ¤ì¼€ì¤„

```bash
# crontab -e
# ë°ì´í„°ë² ì´ìŠ¤ ë°±ì—… (ë§¤ì¼ ìƒˆë²½ 3ì‹œ)
0 3 * * * /opt/dropshipping/scripts/backup-production.sh

# ì• í”Œë¦¬ì¼€ì´ì…˜ ë¡œê·¸ ë°±ì—… (ë§¤ì£¼ ì¼ìš”ì¼)
0 4 * * 0 /opt/dropshipping/scripts/backup-logs.sh

# Redis ìŠ¤ëƒ…ìƒ· (6ì‹œê°„ë§ˆë‹¤)
0 */6 * * * docker exec redis redis-cli BGSAVE
```

### 2. ì¬í•´ ë³µêµ¬ ê³„íš

```bash
#!/bin/bash
# scripts/disaster-recovery.sh

# 1. ìµœì‹  ë°±ì—… í™•ì¸
LATEST_BACKUP=$(aws s3 ls s3://your-backup-bucket/database/ | sort | tail -1 | awk '{print $4}')

# 2. ë°±ì—… ë‹¤ìš´ë¡œë“œ
aws s3 cp s3://your-backup-bucket/database/$LATEST_BACKUP /tmp/

# 3. ë°ì´í„°ë² ì´ìŠ¤ ë³µì›
gunzip /tmp/$LATEST_BACKUP
docker compose -f docker-compose.prod.yml exec -T postgres \
    psql -U $POSTGRES_USER -d $POSTGRES_DB < /tmp/${LATEST_BACKUP%.gz}

# 4. ìºì‹œ ì¬êµ¬ì¶•
docker compose -f docker-compose.prod.yml exec backend \
    python -m app.scripts.rebuild_cache

# 5. í—¬ìŠ¤ì²´í¬
./scripts/health-check.sh
```

## ğŸ”§ íŠ¸ëŸ¬ë¸”ìŠˆíŒ…

### 1. ì¼ë°˜ì ì¸ ë¬¸ì œ í•´ê²°

#### ë©”ëª¨ë¦¬ ë¶€ì¡±
```bash
# ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ í™•ì¸
docker stats

# ì»¨í…Œì´ë„ˆ ë©”ëª¨ë¦¬ ì œí•œ ì¡°ì •
docker update --memory="4g" --memory-swap="4g" backend
```

#### ë””ìŠ¤í¬ ê³µê°„ ë¶€ì¡±
```bash
# Docker ì •ë¦¬
docker system prune -a --volumes

# ì˜¤ë˜ëœ ë¡œê·¸ ì‚­ì œ
find /var/log -name "*.log" -mtime +30 -delete
```

#### ëŠë¦° ë°ì´í„°ë² ì´ìŠ¤ ì¿¼ë¦¬
```sql
-- ëŠë¦° ì¿¼ë¦¬ í™•ì¸
SELECT query, mean_exec_time, calls 
FROM pg_stat_statements 
ORDER BY mean_exec_time DESC 
LIMIT 10;

-- ì¸ë±ìŠ¤ ì¶”ê°€
CREATE INDEX CONCURRENTLY idx_orders_user_created 
ON orders(user_id, created_at);
```

### 2. ê¸´ê¸‰ ëŒ€ì‘ ì ˆì°¨

```bash
#!/bin/bash
# scripts/emergency-response.sh

case "$1" in
  "high-load")
    # ìë™ ìŠ¤ì¼€ì¼ë§
    docker compose -f docker-compose.prod.yml up -d --scale backend=4
    ;;
    
  "database-down")
    # ì½ê¸° ì „ìš© ëª¨ë“œ ì „í™˜
    docker compose -f docker-compose.prod.yml exec backend \
      python -m app.scripts.enable_readonly_mode
    ;;
    
  "security-breach")
    # ëª¨ë“  API í‚¤ ìˆœí™˜
    ./scripts/rotate-api-keys.sh
    # ì˜ì‹¬ìŠ¤ëŸ¬ìš´ IP ì°¨ë‹¨
    ./scripts/block-suspicious-ips.sh
    ;;
esac
```

## âœ… ì²´í¬ë¦¬ìŠ¤íŠ¸

### ë°°í¬ ì „
- [ ] ëª¨ë“  í™˜ê²½ ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì—ˆëŠ”ê°€?
- [ ] SSL ì¸ì¦ì„œê°€ ìœ íš¨í•œê°€?
- [ ] ë°ì´í„°ë² ì´ìŠ¤ ë°±ì—…ì´ ì™„ë£Œë˜ì—ˆëŠ”ê°€?
- [ ] ë¡œë“œ í…ŒìŠ¤íŠ¸ë¥¼ ìˆ˜í–‰í–ˆëŠ”ê°€?
- [ ] ë³´ì•ˆ ìŠ¤ìº”ì„ ì™„ë£Œí–ˆëŠ”ê°€?

### ë°°í¬ ì¤‘
- [ ] í—¬ìŠ¤ì²´í¬ê°€ í†µê³¼í•˜ëŠ”ê°€?
- [ ] ë¡œê·¸ì— ì—ëŸ¬ê°€ ì—†ëŠ”ê°€?
- [ ] ë©”íŠ¸ë¦­ì´ ì •ìƒ ë²”ìœ„ì¸ê°€?
- [ ] íŠ¸ë˜í”½ì´ ì •ìƒì ìœ¼ë¡œ ë¼ìš°íŒ…ë˜ëŠ”ê°€?

### ë°°í¬ í›„
- [ ] ëª¨ë“  ê¸°ëŠ¥ì´ ì •ìƒ ì‘ë™í•˜ëŠ”ê°€?
- [ ] ì„±ëŠ¥ì´ ê¸°ëŒ€ ìˆ˜ì¤€ì¸ê°€?
- [ ] ì•Œë¦¼ì´ ì •ìƒ ì‘ë™í•˜ëŠ”ê°€?
- [ ] ë°±ì—…ì´ ì˜ˆì•½ë˜ì–´ ìˆëŠ”ê°€?
- [ ] ë¬¸ì„œê°€ ì—…ë°ì´íŠ¸ë˜ì—ˆëŠ”ê°€?

## ğŸ“ ë¹„ìƒ ì—°ë½ì²˜

```yaml
on-call:
  primary:
    name: "DevOps Lead"
    phone: "+82-10-XXXX-XXXX"
    email: "devops@company.com"
  
  secondary:
    name: "Backend Lead"
    phone: "+82-10-YYYY-YYYY"
    email: "backend@company.com"

external:
  aws_support: "https://console.aws.amazon.com/support"
  datadog_support: "support@datadog.com"
```

---

ì´ ê°€ì´ë“œë¥¼ ë”°ë¼ ì•ˆì „í•˜ê³  ì•ˆì •ì ì¸ í”„ë¡œë•ì…˜ ë°°í¬ë¥¼ ìˆ˜í–‰í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì •ê¸°ì ì¸ ì—…ë°ì´íŠ¸ì™€ ëª¨ë‹ˆí„°ë§ì„ í†µí•´ ì‹œìŠ¤í…œì˜ ê±´ê°•ì„±ì„ ìœ ì§€í•˜ì„¸ìš”. ğŸš€